# === main.py ===
# main.py
import sys
import agent

def main():
    if len(sys.argv) > 1:
        query = " ".join(sys.argv[1:])
    else:
        query = input("请输入查询: ")
    agent.chat(query)

if __name__ == '__main__':
    main()


# === agent.py ===
#! /usr/bin/env python3
import re
import json
import datetime
import requests
import time
import os  
import traceback
import hashlib
import config 
import dateparser
from django.conf import settings


from analysis.single_series import analyze_single_series
from analysis.multi_series import analyze_multi_series
from output.report_generator import generate_report_single, generate_report_multi
from output.visualization import generate_summary_echarts_html, generate_echarts_html_single, generate_echarts_html_multi

AIOPS_BACKEND_DOMAIN = 'https://aiopsbackend.cstcloud.cn'
LLM_URL = 'http://10.16.1.16:58000/v1/chat/completions'
AUTH = ('chelseyyycheng@outlook.com', 'UofV1uwHwhVp9tcTue')

CACHE_DIR = "cached_data"
os.makedirs(CACHE_DIR, exist_ok=True)

def _cache_filename(ip:str, start_ts:int, end_ts:int, field:str)->str:
    """
    生成缓存文件名（注意此函数的参数顺序必须保持不变）
    
    参数:
        ip: 主机IP
        start_ts: 开始时间戳
        end_ts: 结束时间戳
        field: 指标字段
        
    返回:
        str: 缓存文件路径
    """
    key = f"{ip}_{field}_{start_ts}_{end_ts}"  # 注意字段顺序
    h = hashlib.md5(key.encode('utf-8')).hexdigest()
    return os.path.join(CACHE_DIR, f"{h}.json")

def fetch_data_from_backend(ip:str, start_ts:int, end_ts:int, field:str):
    url = f"{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/metric/format-value/?start={start_ts}&end={end_ts}&instance={ip}&field={field}"
    resp = requests.get(url, auth=AUTH)
    if resp.status_code!=200:
        return f"后端请求失败: {resp.status_code} => {resp.text}"
    j = resp.json()
    results = j.get("results", [])
    if not results:
        return []
    vals = results[0].get("values", [])
    arr = []
    from datetime import datetime
    def parse_ts(s):
        try:
            dt = datetime.strptime(s,"%Y-%m-%d %H:%M:%S")
            return int(dt.timestamp())
        except:
            return 0
    for row in vals:
        if len(row)>=2:
            tstr,vstr = row[0], row[1]
            t = parse_ts(tstr)
            try:
                v = float(vstr)
            except:
                v = 0.0
            arr.append([t,v])
    return arr

def ensure_cache_file(ip:str, start:str, end:str, field:str)->str:
    """
    确保缓存文件存在，如果不存在则从后端获取
    
    参数:
        ip: 主机IP
        start: 开始时间 (格式: "YYYY-MM-DD HH:MM:SS")
        end: 结束时间 (格式: "YYYY-MM-DD HH:MM:SS")
        field: 指标字段
        
    返回:
        str: 缓存文件路径或错误信息
    """
    import datetime
    def to_int(s):
        try:
            dt = datetime.datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
            return int(dt.timestamp())
        except ValueError as e:
            # 处理日期无效的情况
            if "day is out of range for month" in str(e):
                # 找出有效的日期
                parts = s.split(" ")[0].split("-")
                year, month = int(parts[0]), int(parts[1])
                
                # 获取该月的最后一天
                if month == 2:
                    # 检查是否是闰年
                    is_leap = (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)
                    last_day = 29 if is_leap else 28
                elif month in [4, 6, 9, 11]:
                    last_day = 30
                else:
                    last_day = 31
                
                # 构造有效日期字符串
                valid_date = f"{year}-{month:02d}-{last_day:02d} {s.split(' ')[1]}"
                print(f"日期已自动调整: {s} -> {valid_date}")
                
                dt = datetime.datetime.strptime(valid_date, "%Y-%m-%d %H:%M:%S")
                return int(dt.timestamp())
            else:
                # 其他错误直接抛出
                raise
    
    try:
        st_i = to_int(start)
        et_i = to_int(end)
        fpath = _cache_filename(ip, st_i, et_i, field)

        if os.path.exists(fpath):
            print("(已从本地缓存读取)")
            return fpath
        else:
            data = fetch_data_from_backend(ip, st_i, et_i, field)
            if isinstance(data, str):
                return data
            with open(fpath, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print("(已调用后端并写入本地缓存)")
            return fpath
    except Exception as e:
        return f"缓存文件创建失败: {str(e)}"

def load_series_from_cachefile(filepath:str):
    if not os.path.exists(filepath):
        return None
    with open(filepath,"r",encoding="utf-8") as f:
        arr = json.load(f)
    return arr

def parse_time_expressions(raw_text:str):
    segments = re.split(r'[,\uFF0C\u3001\u0026\u002C\u002F\u0020\u0026\u2014\u2013\u2014\u006E\u005E]|和|与|及|还有|、', raw_text)
    results = []
    for seg in segments:
        seg = seg.strip()
        if not seg:
            continue

        dt = dateparser.parse(seg, languages=['zh','en'], settings={"PREFER_DATES_FROM":"past"})
        if dt is None:
            results.append({"start":0, "end":0, "error":f"无法解析: {seg}"})
        else:
            day_s = datetime.datetime(dt.year, dt.month, dt.day, 0,0,0)
            day_e = datetime.datetime(dt.year, dt.month, dt.day, 23,59,59)
            
            # 添加格式化的时间字符串
            start_str = day_s.strftime("%Y-%m-%d %H:%M:%S")
            end_str = day_e.strftime("%Y-%m-%d %H:%M:%S")
            
            results.append({
                "start": int(day_s.timestamp()),
                "end": int(day_e.timestamp()),
                "error": "",
                "start_str": start_str,
                "end_str": end_str
            })
    return results

tools = [
     {
        "name":"解析用户自然语言时间",
        "description":"返回一个list，每个元素是{start, end, error}. 如果不确定，可向用户澄清。",
        "parameters":{
            "type":"object",
            "properties":{
                "raw_text":{"type":"string"}
            },
            "required":["raw_text"]
        }
    },
    {  
        "name": "请求智能运管后端Api，获取指标项的时序数据",
        "description": "从后端或本地缓存获取IP在指定时间范围(field)的时序数据(list of [int_ts, val])。注意start/end必须是形如'YYYY-MM-DD HH:MM:SS'的确定时间。",
        "parameters": {
            "type": "object",
            "properties": {
                "ip": {
                    "type": "string",
                    "description": "要查询的 IP，如 '192.168.0.110'"
                },
                "start": {
                    "type": "string",
                    "description": "开始时间，格式 '2025-03-24 00:00:00'"
                },
                "end": {
                    "type": "string",
                    "description": "结束时间，格式 '2025-03-24 23:59:59'"
                },
                "field": {
                    "type": "string",
                    "description": "监控项名称，如 'cpu_rate'"
                }
            },
            "required": ["ip","start","end","field"]
        }
    },
    {
        "name": "请求智能运管后端Api，查询监控实例有哪些监控项",
        "description": "返回指定IP下可用的监控项列表（可选项）",
        "parameters": {
            "type": "object",
            "properties": {
                "service": {
                    "type": "string",
                    "description": "系统服务名称 (一般填 '主机监控')"
                },
                "instance": {
                    "type": "string",
                    "description": "监控实例 IP"
                }
            },
            "required": ["service","instance"]
        }
    },
    {
        "name": "请求智能运管后端Api，查询监控服务的资产情况和监控实例",
        "description": "查询一个监控服务的所有资产/IP等信息",
        "parameters": {
            "type": "object",
            "properties": {
                "service": {
                    "type": "string",
                    "description": "要查询的系统服务名称"
                }
            },
            "required": ["service"]
        }
    },
    {
        "name": "请求智能运管后端Api，查询监控实例之间的拓扑关联关系",
        "description": "查询指定IP的上联、下联监控实例等信息",
        "parameters": {
            "type": "object",
            "properties": {
                "service": {
                    "type": "string",
                    "description": "系统服务名称"
                },
                "instance_ip": {
                    "type": "string",
                    "description": "监控实例IP"
                }
            },
            "required": ["service","instance_ip"]
        }
    },
    {
        "name": "单序列异常检测(文件)",
        "description": "对单序列 [int_ts,val] 进行多方法分析, 生成报告和ECharts HTML",
        "parameters": {
            "type": "object",
            "properties": {
                "ip":    {"type": "string"},
                "field": {"type": "string"},
                "start": {"type": "string"},
                "end":   {"type": "string"}
            },
            "required": ["ip","field","start","end"]
        }
    },
    {
        "name": "多序列对比异常检测(文件)",
        "description": "对两组 [int_ts,val] 进行对比分析, 生成报告和ECharts HTML",
        "parameters": {
            "type": "object",
            "properties": {
                "ip1":    {"type": "string"},
                "field1": {"type": "string"},
                "start1": {"type": "string"},
                "end1":   {"type": "string"},
                "ip2":    {"type": "string"},
                "field2": {"type": "string"},
                "start2": {"type": "string"},
                "end2":   {"type": "string"}
            },
            "required": ["ip1","field1","start1","end1","ip2","field2","start2","end2"]
        }
    }
]

def monitor_item_list(ip):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/machine/field/?instance={ip}'
    resp = requests.get(url=url, auth=AUTH)
    if resp.status_code == 200:
        items = json.loads(resp.text)
        result = {}
        for x in items:
            result[x.get('field')] = x.get('purpose')
        return result
    else:
        return f"查询监控项失败: {resp.status_code} => {resp.text}"

def get_service_asset(service):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/property/mail/?ordering=num_id&page=1&page_size=2000'
    resp = requests.get(url=url, auth=AUTH)
    if resp.status_code == 200:
        text = json.loads(resp.text)
        results = text.get('results',[])
        item_list = []
        for r in results:
            r["category"] = r.get("category",{}).get("name")
            r["ip_set"] = [_.get("ip") for _ in r.get('ip_set',[])]
            for k in ["num_id","creation","modification","remark","sort_weight","monitor_status"]:
                r.pop(k, None)
            for k,v in list(r.items()):
                if not v or v == "无":
                    r.pop(k)
            item_list.append(r)
        return item_list
    else:
        return f"查询失败: {resp.status_code} => {resp.text}"

def get_service_asset_edges(service, instance_ip):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/property/mail/topology/search?instance={instance_ip}'
    resp = requests.get(url=url, auth=AUTH)
    if resp.status_code == 200:
        return json.loads(resp.text)
    else:
        return f"查询拓扑失败: {resp.status_code} => {resp.text}"
    
def get_monitor_metric_value(ip, start, end, field):
    import datetime
    def to_int(s):
        dt = datetime.datetime.strptime(s,"%Y-%m-%d %H:%M:%S")
        return int(dt.timestamp())
    st_i = to_int(start)
    et_i = to_int(end)
    fpath= _cache_filename(ip, st_i, et_i, field)
    if os.path.exists(fpath):
        print("(已从本地缓存读取)")
        return json.load(open(fpath,"r",encoding="utf-8"))
    else:
        data= fetch_data_from_backend(ip, st_i, et_i, field)
        if isinstance(data,str):
            return data
        with open(fpath,"w",encoding="utf-8") as f:
            json.dump(data,f,ensure_ascii=False,indent=2)
        print("(已调用后端并写入本地缓存)")
        return data

###############################################################################
def single_series_detect(ip, field, start, end):
    fpath = ensure_cache_file(ip, start, end, field)
    if isinstance(fpath, str) and not os.path.exists(fpath):
        return {"error": fpath}  

    if os.path.exists(fpath):
        series = load_series_from_cachefile(fpath)
        if series is None:
            return {"error": f"无法加载缓存文件: {fpath}"}

        try:
            user_query = f"分析 {ip} 在 {start} ~ {end} 的 {field} 数据"
            result = generate_report_single(series, ip, field, user_query)
            return result
        except Exception as e:
            print(f"单序列分析生成报告失败: {e}")
            traceback.print_exc()  # 打印详细错误
            # 通过旧方法返回基本结果
            analysis_result = analyze_single_series(series)
            return {
                "classification": analysis_result["classification"],
                "composite_score": analysis_result["composite_score"],
                "anomaly_times": analysis_result["anomaly_times"],
                "report_path": "N/A - 报告生成失败"
            }

    return {"error": fpath}

def multi_series_detect(ip1, field1, start1, end1, ip2, field2, start2, end2):
    fpath1 = ensure_cache_file(ip1, start1, end1, field1)
    fpath2 = ensure_cache_file(ip2, start2, end2, field2)
    
    if isinstance(fpath1, str) and not os.path.exists(fpath1):
        return {"error": fpath1}
    if isinstance(fpath2, str) and not os.path.exists(fpath2):
        return {"error": fpath2}

    series1 = load_series_from_cachefile(fpath1)
    series2 = load_series_from_cachefile(fpath2)
    if series1 is None or series2 is None:
        return {"error": f"无法加载本地缓存文件: {fpath1} / {fpath2}"}

    try:
        user_query = f"对比分析 {ip1} 在 {start1} 和 {start2} 的 {field1} 指标"
        result = generate_report_multi(series1, series2, ip1, ip2, field1, user_query)
        return result
    except Exception as e:
        print(f"多序列分析生成报告失败: {e}")
        traceback.print_exc()  # 打印详细错误
        # 通过旧方法返回基本结果
        analysis_result = analyze_multi_series(series1, series2)
        return {
            "classification": analysis_result["classification"],
            "composite_score": analysis_result["composite_score"],
            "anomaly_times": analysis_result["anomaly_times"],
            "anomaly_intervals": analysis_result.get("anomaly_intervals", []),
            "report_path": "N/A - 报告生成失败"
        }

###############################################################################

def llm_call(messages):
    data={
      "model":"Qwen2.5-14B-Instruct",
      "temperature":0.1,
      "messages":messages
    }
    r= requests.post(LLM_URL, json=data)
    if r.status_code==200:
        jj= r.json()
        if "choices" in jj and len(jj["choices"])>0:
            return jj["choices"][0]["message"]
        else:
            return None
    else:
        print("Error:", r.status_code, r.text)
        return None
    
def init_msg(role, content):
    return {"role": role, "content": content}


def parse_llm_response(txt):
    pat_thought = r"<思考过程>(.*?)</思考过程>"
    pat_action  = r"<工具调用>(.*?)</工具调用>"
    pat_inparam = r"<调用参数>(.*?)</调用参数>"
    pat_final   = r"<最终答案>(.*?)</最终答案>"
    pat_supplement = r"<补充请求>(.*?)</补充请求>"
    def ext(pattern):
        m = re.search(pattern, txt, flags=re.S)
        return m.group(1) if m else ""

    return {
        "thought": ext(pat_thought),
        "action":  ext(pat_action),
        "action_input": ext(pat_inparam),
        "final_answer": ext(pat_final),
        "supplement": ext(pat_supplement)
    }

def react(llm_text):
    parsed = parse_llm_response(llm_text)
    action = parsed["action"]
    inp_str = parsed["action_input"]
    final_ans = parsed["final_answer"]
    supplement = parsed["supplement"]
    is_final = False

    if action and inp_str:
        try:
            action_input = json.loads(inp_str)
        
        # 检查是否存在时间占位符
            for key in ["start", "end"]:
                if key in action_input and isinstance(action_input[key], str):
                    value = action_input[key]
                # 检测占位符文本
                    if "解析结果" in value or "待定" in value:
                    # 提示用户指定时间
                        return f"请提供具体的{key}时间（格式：YYYY-MM-DD HH:MM:SS）", False
        except:
            return f"无法解析调用参数JSON: {inp_str}", False

        if action == "单序列异常检测(文件)":
            result = single_series_detect(**action_input)
            if "error" in result:
                return result["error"], False

            rep = result
            # 添加文字分析报告
            analysis_text = rep.get("analysis_text", "")
    
            final_answer = f"""
## 单序列异常检测报告

**结论**：{rep['classification']}  
**得分**：{rep['composite_score']:.2f}  
**异常点数**：{len(rep['anomaly_times'])}

{analysis_text}

**图表路径**：{rep['report_path']}
"""
            return final_answer, False

        elif action == "多序列对比异常检测(文件)":
            result = multi_series_detect(**action_input)
            if "error" in result:
                return result["error"], False

            rep = result
            # 添加文字分析报告到返回结果
            analysis_text = rep.get("analysis_text", "")
    
            final_answer = f"""
## 多序列对比异常检测报告

**结论**：{rep['classification']}  
**综合得分**：{rep['composite_score']:.2f}  
**异常段数**：{len(rep.get('anomaly_intervals', []))}

{analysis_text}

**图表路径**：{rep['report_path']}
"""
            return final_answer, False
        
        # 其他 action 保持不变
        elif action == "解析用户自然语言时间":
            return parse_time_expressions(action_input["raw_text"]), False
        elif action == "请求智能运管后端Api，获取指标项的时序数据":
            return get_monitor_metric_value(**action_input), False
        elif action == "请求智能运管后端Api，查询监控实例有哪些监控项":
            return monitor_item_list(action_input["instance"]), False
        elif action == "请求智能运管后端Api，查询监控服务的资产情况和监控实例":
            return get_service_asset(action_input["service"]), False
        elif action == "请求智能运管后端Api，查询监控实例之间的拓扑关联关系":
            return get_service_asset_edges(action_input["service"], action_input["instance_ip"]), False
        else:
            return f"未知工具调用: {action}", False
        
    if supplement.strip():
        return {"type": "supplement", "content": supplement}

    if final_ans.strip():
        is_final = True
        return final_ans, is_final

    return ("格式不符合要求，必须使用：<思考过程></思考过程> <工具调用></工具调用> <调用参数></调用参数> <最终答案></最终答案>", is_final)

def shorten_tool_result(res):
    if isinstance(res, list):
        # 特殊处理时间解析结果
        if len(res) > 0 and isinstance(res[0], dict) and "start" in res[0] and "end" in res[0]:
            # 添加格式化的时间字符串
            for item in res:
                if "error" not in item or not item["error"]:
                    # 如果不存在start_str和end_str，添加它们
                    if "start_str" not in item:
                        start_dt = datetime.datetime.fromtimestamp(item["start"])
                        item["start_str"] = start_dt.strftime("%Y-%m-%d %H:%M:%S") 
                    if "end_str" not in item:
                        end_dt = datetime.datetime.fromtimestamp(item["end"])
                        item["end_str"] = end_dt.strftime("%Y-%m-%d %H:%M:%S")
            
            # 生成一个更易读的摘要
            time_results = []
            for item in res:
                if "error" in item and item["error"]:
                    time_results.append({"error": item["error"]})
                else:
                    time_results.append({
                        "start_time": item.get("start_str", ""),
                        "end_time": item.get("end_str", ""),
                        "start": item.get("start", 0),
                        "end": item.get("end", 0)
                    })
            return json.dumps(time_results, ensure_ascii=False)
        return f"[List len={len(res)}]"
    # 其他类型的响应保持不变
    elif isinstance(res, dict):
        summary = {}
        for k,v in res.items():
            if isinstance(v, list):
                summary[k] = f"[List len={len(v)}]"
            elif isinstance(v, str) and len(v)>300:
                summary[k] = v[:300] + f"...(omitted, length={len(v)})"
            else:
                summary[k] = v
        return json.dumps(summary, ensure_ascii=False)
    elif isinstance(res, str) and len(res)>300:
        return res[:300] + f"...(omitted, length={len(res)})"
    else:
        return str(res)

def chat(user_query):
    system_prompt = f'''你是一个严格遵守格式规范的用于运维功能，运维数据可视化，运行于生产环境的ReAct智能体，你叫小助手，必须按以下格式处理请求：

    你的工具列表如下:
    {json.dumps(tools, ensure_ascii=False, indent=2)}
    当前时间为: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

    处理规则：
    1. 时间推理能力: 
       当用户提到相对时间（如"上周星期一"、"上上周星期一"、"昨天"等），你必须根据当前时间自动计算出对应的具体日期，不要向用户提问确认。
       -03-31（星期一），那么"上周星期一"就是2025-03-24，"上上周星期一"就是2025-03-17，"昨天"就是2025-03-30。
       你应该具备日历计算能力，能够正确处理月末、闰年等特殊情况。
       只有在时间表达式真正存在歧义（无法通过上下文或常识推断）时，才询问用户进一步澄清。
       始终使用YYYY-MM-DD HH:MM:SS格式作为工具调用的参数。

    2. 分析任务决策:
       如果用户提供或暗示了1个时间区间，则调用'单序列异常检测(文件)'。
       如果用户提供或暗示了多个时间区间，并且用户输入包含"对比"、"相比"、"比较"、"环比"、"VS"、"vs"、"变化"、"相较于"等明显比较词汇，则调用'多序列对比异常检测(文件)'。
       如果用户提供或暗示了多个时间区间，但没有明确表示是要比较还是单独分析，在<补充请求>中询问用户意图。
       如果用户要求分析多个不同指标或不同主机，应该分别进行分析，而不是拼接到一起。

    3. 工作流程:
       你每次只能调用一个工具，不能在同一次响应中调用多个工具。如果有多个任务，请分轮执行。
       尽量减少不必要的补充请求。对用户意图应该有合理推断，不要过度询问。
       不能伪造数据。如果某些数据无法获取，要清楚地告知用户。
       对于时间表达式的解析，使用你自己的推理能力，而不仅仅依赖"解析用户自然语言时间"工具。

    4.严格按照以下xml格式生成响应文本：
    ```
    <思考过程>你的思考过程</思考过程>
    <工具调用>工具名称，不调用则为空</工具调用>
    <调用参数>工具输入参数{{json}}</调用参数>
    <最终答案>用户问题的最终结果（知道问题的最终答案时返回）</最终答案>
    <补充请求>系统请求用户补充信息</补充请求>
    ```
    '''
    history=[]
    history.append({"role":"system","content":system_prompt})
    history.append({"role":"user","content": user_query})

    round_num=1
    max_round=15
    pending_context = None 
    had_supplement = False

    while True:
        print(f"=== 第{round_num}轮对话 ===")

        if pending_context:
            ans = llm_call(pending_context["history"])
            pending_context = None  
        else:
            ans = llm_call(history)
            
        if not ans:
            print("大模型返回None,结束")
            return

        #print("## 大模型完整响应:", ans)
        print(ans["content"])

        history.append(ans)
        txt= ans.get("content","")
        res = react(txt)

        if isinstance(res, dict) and res.get("type") == "supplement":
            print(f"\n小助手: {res['content']}")
            user_input = input("你: ")
            
            supplement_response = f"对于您的问题 '{res['content']}'，我的回答是: {user_input}"
            history.append({"role": "user", "content": supplement_response})
            
            had_supplement = True
            round_num += 1
            continue

        result, done = res
        
        short_result = shorten_tool_result(result)
        history.append({
            "role":"user",
            "content": f"<工具调用结果>: {short_result}"
        })

        if done:
            print("===最终输出===")
            print(result)
            return

        round_num+=1
        if round_num>max_round:
            print("超出上限")
            return



if __name__ == '__main__':
    # chat('你好')
    chat(
        '请分析192.168.0.110这台主机上周星期一和上上周星期一的cpu利用率还有昨天的用户cpu利用率，并作图给出分析报告')


# === config.py ===
# config.py
import json
import os
import logging

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("anomaly_detection")

# 各个检测方法的权重配置 - 调整权重使得结果更合理
WEIGHTS_SINGLE = {
    "Z-Score": 0.5,
    "CUSUM": 0.5
}

WEIGHTS_MULTI = {
    "ResidualComparison": 0.4,  # 提高残差对比的权重
    "TrendDriftCUSUM": 0.25,    # 降低CUSUM的权重
    "ChangeRate": 0.15,
    "TrendSlope": 0.2
}

# 综合分数阈值 - 提高阈值，减少误报
HIGH_ANOMALY_THRESHOLD = 0.7
MILD_ANOMALY_THRESHOLD = 0.4

# 默认的各检测方法阈值配置 - 更加严格的阈值
DEFAULT_THRESHOLD_CONFIG = {
    "Z-Score": {
        "threshold": 3.5  # 增大Z-Score阈值，减少误检
    },
    "CUSUM": {
        "drift_threshold": 6.0,  # 增大偏移阈值
        "k": 0.7  # 增大k值，减少对小波动的敏感度
    },
    "ResidualComparison": {
        "threshold": 3.5
    },
    "TrendDriftCUSUM": {
        "drift_threshold": 8.0  # 显著增大阈值减少误报
    },
    "ChangeRate": {
        "threshold": 0.7  # 增大变化率阈值
    },
    "TrendSlope": {
        "slope_threshold": 0.4,  # 增大斜率阈值
        "window": 5
    }
}

# 创建配置目录
CONFIG_DIR = "config"
os.makedirs(CONFIG_DIR, exist_ok=True)
THRESHOLD_CONFIG_PATH = os.path.join(CONFIG_DIR, "threshold_config.json")

# 如果不存在配置文件，创建一个默认配置
if not os.path.exists(THRESHOLD_CONFIG_PATH):
    try:
        with open(THRESHOLD_CONFIG_PATH, "w", encoding="utf-8") as f:
            json.dump(DEFAULT_THRESHOLD_CONFIG, f, ensure_ascii=False, indent=2)
        logger.info(f"已创建默认阈值配置文件: {THRESHOLD_CONFIG_PATH}")
    except Exception as e:
        logger.warning(f"无法创建默认配置文件: {e}")

# 加载用户定义的阈值配置
try:
    with open(THRESHOLD_CONFIG_PATH, "r", encoding="utf-8") as f:
        USER_THRESHOLD_CONFIG = json.load(f)
        # 合并用户配置与默认配置
        THRESHOLD_CONFIG = DEFAULT_THRESHOLD_CONFIG.copy()
        for method, config in USER_THRESHOLD_CONFIG.items():
            if method in THRESHOLD_CONFIG:
                THRESHOLD_CONFIG[method].update(config)
            else:
                THRESHOLD_CONFIG[method] = config
        logger.info(f"已加载用户阈值配置: {THRESHOLD_CONFIG_PATH}")
except Exception as e:
    logger.warning(f"无法读取阈值配置文件，使用默认值: {e}")
    THRESHOLD_CONFIG = DEFAULT_THRESHOLD_CONFIG

# 数据库配置
DB_CONFIG = {
    'HOST': 'localhost',
    'PORT': 3306,
    'USER': 'aiops',
    'PASSWORD': 'aiops123',
    'NAME': 'aiops_monitoring'
}

# === utils/ts_cache.py ===
# utils/ts_cache.py
import os
import pickle
import hashlib
import json
import requests
import datetime
from typing import List, Tuple, Optional, Dict, Any

# 配置
CACHE_DIR = "cached_data"
os.makedirs(CACHE_DIR, exist_ok=True)

# 后端API相关配置
AIOPS_BACKEND_DOMAIN = 'https://aiopsbackend.cstcloud.cn'
AUTH = ('chelseyyycheng@outlook.com', 'UofV1uwHwhVp9tcTue')

def _cache_filename(ip: str, field: str, start_ts: int, end_ts: int) -> str:
    """
    生成缓存文件名
    
    参数:
        ip: 主机IP
        field: 指标字段
        start_ts: 开始时间戳
        end_ts: 结束时间戳
    
    返回:
        str: 缓存文件路径
    """
    key = f"{ip}_{field}_{start_ts}_{end_ts}"
    h = hashlib.md5(key.encode('utf-8')).hexdigest()
    return os.path.join(CACHE_DIR, f"{h}.pkl")

def fetch_data_from_backend(ip: str, field: str, start_ts: int, end_ts: int) -> List[Tuple[int, float]]:
    """
    从后端API获取时序数据
    
    参数:
        ip: 主机IP
        field: 指标字段
        start_ts: 开始时间戳
        end_ts: 结束时间戳
    
    返回:
        List[Tuple[int, float]]: 时间戳-值的列表
    """
    url = f"{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/metric/format-value/?start={start_ts}&end={end_ts}&instance={ip}&field={field}"
    resp = requests.get(url, auth=AUTH)
    
    if resp.status_code != 200:
        raise Exception(f"后端请求失败: {resp.status_code} => {resp.text}")
    
    j = resp.json()
    results = j.get("results", [])
    if not results:
        return []
    
    vals = results[0].get("values", [])
    arr = []
    
    def parse_ts(s):
        try:
            dt = datetime.datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
            return int(dt.timestamp())
        except:
            return 0
    
    for row in vals:
        if len(row) >= 2:
            tstr, vstr = row[0], row[1]
            t = parse_ts(tstr)
            try:
                v = float(vstr)
            except:
                v = 0.0
            arr.append([t, v])
    
    return arr

def ensure_cache_file(ip: str, field: str, start: str, end: str) -> str:
    """
    确保缓存文件存在，如果不存在则从后端获取
    
    参数:
        ip: 主机IP
        field: 指标字段
        start: 开始时间 (格式: "YYYY-MM-DD HH:MM:SS")
        end: 结束时间 (格式: "YYYY-MM-DD HH:MM:SS")
    
    返回:
        str: 缓存文件路径
    """
    # 转换时间字符串为时间戳
    def to_int(s):
        dt = datetime.datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
        return int(dt.timestamp())
    
    st_i = to_int(start)
    et_i = to_int(end)
    
    # 生成缓存文件路径
    fpath = _cache_filename(ip, field, st_i, et_i)
    
    # 检查缓存是否存在
    if os.path.exists(fpath):
        print(f"[缓存] 从本地缓存读取 {ip} {field} 数据")
        return fpath
    
    # 从后端获取数据
    try:
        print(f"[API] 从后端获取 {ip} {field} 数据")
        data = fetch_data_from_backend(ip, field, st_i, et_i)
        
        # 保存到缓存
        with open(fpath, "wb") as f:
            pickle.dump(data, f)
        
        print(f"[缓存] 数据已写入缓存 {fpath}")
        return fpath
    
    except Exception as e:
        print(f"[错误] 获取数据失败: {e}")
        return str(e)

def load_series_from_cache(ip: str, field: str, start: str, end: str) -> List[Tuple[int, float]]:
    """
    从缓存加载时序数据
    
    参数:
        ip: 主机IP
        field: 指标字段
        start: 开始时间 (格式: "YYYY-MM-DD HH:MM:SS")
        end: 结束时间 (格式: "YYYY-MM-DD HH:MM:SS")
    
    返回:
        List[Tuple[int, float]]: 时间戳-值的列表
    """
    # 确保缓存文件存在
    cache_file = ensure_cache_file(ip, field, start, end)
    
    # 检查路径是否为错误信息
    if not os.path.exists(cache_file):
        raise Exception(f"缓存文件不存在: {cache_file}")
    
    # 从缓存加载数据
    with open(cache_file, "rb") as f:
        data = pickle.load(f)
    
    return data

# === utils/__init__.py ===
# utils/__init__.py
"""
工具函数模块，包含各种通用辅助函数
"""
from utils.time_utils import group_anomaly_times
from utils.ts_cache import ensure_cache_file, load_series_from_cache

# === utils/time_utils.py ===
# utils/time_utils.py
def group_anomaly_times(anomalies, max_gap=1800):
    """
    将时间戳列表分组为连续的时间区间
    
    参数:
        anomalies: 时间戳列表
        max_gap: 允许的最大间隔秒数
        
    返回:
        list: [(start1, end1), (start2, end2), ...] 区间列表
    """
    if not anomalies:
        return []
    
    # 排序输入
    sorted_anomalies = sorted(anomalies)
    
    intervals = []
    cur_start = sorted_anomalies[0]
    cur_end = sorted_anomalies[0]
    
    for t in sorted_anomalies[1:]:
        if t - cur_end <= max_gap:
            cur_end = t
        else:
            intervals.append((cur_start, cur_end))
            cur_start = t
            cur_end = t
    
    # 添加最后一个区间
    intervals.append((cur_start, cur_end))
    
    return intervals

# === output/report_generator.py ===
# output/report_generator.py
import os
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional

import config
from output.visualization import generate_summary_echarts_html

def generate_text_analysis(detection_results, composite_score, classification, series_info, is_multi_series=False):
    """
    生成易于理解的文字分析报告
    
    参数:
        detection_results: 检测结果列表
        composite_score: 综合得分
        classification: 分类结果 ('正常', '轻度异常', '高置信度异常')
        series_info: 关于数据的信息 {"ip": str, "field": str, "start": str, "end": str}
        is_multi_series: 是否是多序列分析
        
    返回:
        str: 结构化的文字分析报告
    """
    # 提取基本信息
    ip = series_info.get("ip", "未知主机")
    field = series_info.get("field", "未知指标")
    start_time = series_info.get("start", "未知开始时间")
    end_time = series_info.get("end", "未知结束时间")
    
    # 检测统计信息
    total_anomalies = sum(len(result.anomalies) for result in detection_results)
    total_intervals = sum(len(result.intervals) for result in detection_results)
    methods_with_anomalies = [r.method for r in detection_results if len(r.anomalies) > 0 or len(r.intervals) > 0]
    
    # 格式化时间
    try:
        start_dt = datetime.strptime(start_time, "%Y-%m-%d %H:%M:%S")
        end_dt = datetime.strptime(end_time, "%Y-%m-%d %H:%M:%S")
        period_desc = f"{start_dt.strftime('%Y年%m月%d日 %H:%M')} 至 {end_dt.strftime('%Y年%m月%d日 %H:%M')}"
    except:
        period_desc = f"{start_time} 至 {end_time}"
    
    # 根据评分生成总体分析
    if classification == "正常":
        summary_line = f"系统运行正常，未发现明显异常。"
        if total_anomalies > 0:
            summary_line += f" 虽然检测到 {total_anomalies} 个可能的异常点，但整体影响较小，综合评分为 {composite_score:.2f}，低于异常阈值。"
    elif classification == "轻度异常":
        summary_line = f"系统运行存在轻微异常，综合评分为 {composite_score:.2f}。"
        if total_anomalies > 0:
            summary_line += f" 共检测到 {total_anomalies} 个异常点，可能需要关注。"
    else:  # 高置信度异常
        summary_line = f"系统运行存在明显异常，综合评分为 {composite_score:.2f}，超过高置信阈值。"
        if total_anomalies > 0:
            summary_line += f" 共检测到 {total_anomalies} 个异常点，强烈建议进一步分析。"
    
    # 构建详细分析报告
    if is_multi_series:
        ip1 = series_info.get("ip", "主机")
        ip2 = series_info.get("ip2", "主机")
        field = series_info.get("field", "指标")
        
        # 更具体的多序列分析摘要
        analysis_text = f"""## {ip1} {field} 对比分析报告

**分析时段**: {period_desc}

**总体分析**: {summary_line}

**对比结果摘要**: 对比分析了两个时间段的{field}数据，{"发现明显的差异模式" if classification != "正常" else "两个序列整体模式相似"}。
"""
        if total_anomalies > 0:
            analysis_text += f"共检测到 {total_anomalies} 个异常点和 {total_intervals} 个异常区间。\n"
        
        analysis_text += "\n**检测方法详情**:\n"
    else:
        analysis_text = f"""## {ip} {field} 异常检测报告

**分析时段**: {period_desc}

**总体分析**: {summary_line}

**检测方法详情**:
"""

    # 添加各个检测方法的详细分析
    for result in detection_results:
        # 跳过无异常的方法
        if not result.anomalies and not result.intervals:
            continue
            
        method_desc = result.description
        
        # 处理不同类型的异常
        if result.visual_type == "point" and result.anomalies:
            analysis_text += f"\n- **{result.method}**: 检测到 {len(result.anomalies)} 个异常点。"
            
            # 添加代表性异常的解释
            if result.explanation and len(result.explanation) > 0:
                max_examples = min(3, len(result.explanation))
                analysis_text += " 典型异常表现为: "
                for i in range(max_examples):
                    # 尝试将时间戳转换为可读时间
                    try:
                        ts = result.anomalies[i]
                        ts_str = datetime.fromtimestamp(ts).strftime("%H:%M:%S")
                        analysis_text += f"{ts_str} - {result.explanation[i]}; "
                    except:
                        analysis_text += f"{result.explanation[i]}; "
        
        elif result.visual_type in ("range", "curve") and result.intervals:
            analysis_text += f"\n- **{result.method}**: 检测到 {len(result.intervals)} 个异常区间。"
            
            # 添加代表性区间的解释
            if result.explanation and len(result.explanation) > 0:
                max_examples = min(2, len(result.explanation))
                analysis_text += " 典型异常区间表现为: "
                for i in range(max_examples):
                    try:
                        start, end = result.intervals[i]
                        start_str = datetime.fromtimestamp(start).strftime("%H:%M:%S")
                        end_str = datetime.fromtimestamp(end).strftime("%H:%M:%S")
                        analysis_text += f"{start_str}至{end_str} - {result.explanation[i]}; "
                    except:
                        analysis_text += f"{result.explanation[i]}; "
        
        elif result.visual_type == "none" and result.explanation:
            analysis_text += f"\n- **{result.method}**: {' '.join(result.explanation[:2])}"
    
    # 添加建议和结论
    analysis_text += "\n\n**建议**:\n"
    
    if classification == "正常":
        analysis_text += "- 系统运行状态良好，可继续当前运维策略。"
    elif classification == "轻度异常":
        analysis_text += f"- 关注 {field} 指标的变化趋势，尤其是 {', '.join(methods_with_anomalies[:2])} 检测到的异常点。\n"
        analysis_text += "- 考虑设置该指标的监控告警，避免问题恶化。"
    else:  # 高置信度异常
        analysis_text += f"- 建议立即排查 {field} 指标异常的根本原因，特别是 {', '.join(methods_with_anomalies[:2])} 检测到的问题。\n"
        analysis_text += "- 检查系统配置和相关依赖组件状态。\n"
        analysis_text += "- 考虑与其他指标交叉分析，确定影响范围。"
    
    return analysis_text

def generate_report_single(series, ip, field, user_query):
    """
    为单序列异常检测生成汇总报告
    
    参数:
        series: 时序数据 [(ts, value), ...]
        ip: 主机IP
        field: 指标字段
        user_query: 用户查询
    
    返回:
        dict: 报告结果包含图表路径和分析结果
    """
    # 执行异常检测
    from analysis.single_series import analyze_single_series
    result = analyze_single_series(series)
    method_results = result["method_results"]
    
    # 创建输出目录
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_dir = f"output/plots/{ip}_{field}_{timestamp}"
    os.makedirs(base_dir, exist_ok=True)
    
    # 提取时间信息构建 series_info
    start_time = datetime.fromtimestamp(series[0][0]).strftime("%Y-%m-%d %H:%M:%S") if series else "未知"
    end_time = datetime.fromtimestamp(series[-1][0]).strftime("%Y-%m-%d %H:%M:%S") if series else "未知"
    
    series_info = {
        "ip": ip,
        "field": field,
        "start": start_time,
        "end": end_time
    }
    
    # 生成分析报告文字
    analysis_text = generate_text_analysis(
        method_results, 
        result["composite_score"], 
        result["classification"], 
        series_info,
        is_multi_series=False
    )
    
    # 生成汇总图表
    summary_path = os.path.join(base_dir, "summary.html")
    chart_path, tooltip_map = generate_summary_echarts_html(
        series, None, method_results, summary_path, 
        title=f"{ip} {field} 异常检测汇总图"
    )
    
    # 生成最终报告
    final_report_path = os.path.join(base_dir, "final_report.html")
    generate_report_single_series(
        user_query, chart_path, method_results, tooltip_map, final_report_path, 
        analysis_text=analysis_text, composite_score=result["composite_score"],
        classification=result["classification"]  # 确保传递分类和得分
    )
    
    # 返回结果
    return {
        "classification": result["classification"],
        "composite_score": result["composite_score"],
        "anomaly_times": result["anomaly_times"],
        "method_results": [mr.to_dict() for mr in method_results],
        "report_path": final_report_path,
        "analysis_text": analysis_text
    }

def generate_report_multi(series1, series2, ip1, ip2, field, user_query):
    """
    为多序列对比异常检测生成汇总报告
    
    参数:
        series1: 第一个时序数据 [(ts, value), ...]
        series2: 第二个时序数据 [(ts, value), ...]
        ip1: 第一个主机IP
        ip2: 第二个主机IP
        field: 指标字段
        user_query: 用户查询
    
    返回:
        dict: 报告结果包含图表路径和分析结果
    """
    # 执行多序列对比异常检测
    from analysis.multi_series import analyze_multi_series
    result = analyze_multi_series(series1, series2)
    method_results = result["method_results"]
    
    # 创建输出目录
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_dir = f"output/plots/{ip1}_{ip2}_{field}_{timestamp}"
    os.makedirs(base_dir, exist_ok=True)
    
    # 提取时间信息构建 series_info
    start_time = datetime.fromtimestamp(series1[0][0]).strftime("%Y-%m-%d %H:%M:%S") if series1 else "未知"
    end_time = datetime.fromtimestamp(series1[-1][0]).strftime("%Y-%m-%d %H:%M:%S") if series1 else "未知"
    
    series_info = {
        "ip": ip1,
        "ip2": ip2,
        "field": field,
        "start": start_time,
        "end": end_time
    }
    
    # 生成分析报告文字
    analysis_text = generate_text_analysis(
        method_results, 
        result["composite_score"], 
        result["classification"], 
        series_info,
        is_multi_series=True
    )
    
    # 生成汇总图表 - 传递两个序列
    summary_path = os.path.join(base_dir, "summary.html")
    chart_path, tooltip_map = generate_summary_echarts_html(
        series1, series2, method_results, summary_path,
        title=f"{ip1} vs {ip2} {field} 对比异常检测汇总图"
    )
    
    # 生成最终报告
    final_report_path = os.path.join(base_dir, "final_report.html")
    generate_report_single_series(
        user_query, chart_path, method_results, tooltip_map, final_report_path,
        analysis_text=analysis_text, composite_score=result["composite_score"], 
        classification=result["classification"]  # 确保传递分类和得分
    )
    
    # 返回结果
    return {
        "classification": result["classification"],
        "composite_score": result["composite_score"],
        "anomaly_times": result["anomaly_times"],
        "anomaly_intervals": result["anomaly_intervals"],
        "method_results": [mr.to_dict() for mr in method_results],
        "report_path": final_report_path,
        "analysis_text": analysis_text
    }
def generate_report_single_series(user_query, chart_path, detection_results, tooltip_map, output_path, analysis_text=None, composite_score=0.0, classification="正常"):
    """
    生成单序列异常检测HTML报告
    
    参数:
        user_query: 用户原始查询
        chart_path: 图表HTML路径
        detection_results: 检测结果列表
        tooltip_map: 异常点/区间映射
        output_path: 输出HTML报告路径
        analysis_text: 可选，分析文本报告
        composite_score: 综合得分
        classification: 分类结果
    
    返回:
        str: 生成的报告路径
    """
    # 使用传入的综合评分和分类，而不是重新计算
    # 设置报告样式类
    if classification == "高置信度异常":
        alert_class = "alert-danger"
    elif classification == "轻度异常":
        alert_class = "alert-warning"
    else:
        alert_class = "alert-success"
    
    # 构建异常点说明HTML
    anomaly_explanations_html = ""
    for point_id, info in tooltip_map.items():
        # 格式化时间戳
        if "ts" in info:
            time_str = datetime.fromtimestamp(info["ts"]).strftime("%Y-%m-%d %H:%M:%S")
            anomaly_explanations_html += f"""
            <div class="card mb-2">
                <div class="card-header">
                    <strong>异常点 #{point_id}</strong> - {time_str}
                </div>
                <div class="card-body">
                    <p><strong>方法:</strong> {info["method"]}</p>
                    <p><strong>值:</strong> {info.get("value", "N/A")}</p>
                    <p><strong>说明:</strong> {info["explanation"]}</p>
                </div>
            </div>
            """
        # 区间异常
        elif "ts_start" in info and "ts_end" in info:
            start_str = datetime.fromtimestamp(info["ts_start"]).strftime("%Y-%m-%d %H:%M:%S")
            end_str = datetime.fromtimestamp(info["ts_end"]).strftime("%Y-%m-%d %H:%M:%S")
            anomaly_explanations_html += f"""
            <div class="card mb-2">
                <div class="card-header">
                    <strong>异常区间 #{point_id}</strong> - {start_str} 至 {end_str}
                </div>
                <div class="card-body">
                    <p><strong>方法:</strong> {info["method"]}</p>
                    <p><strong>说明:</strong> {info["explanation"]}</p>
                </div>
            </div>
            """
    
    # 如果没有异常，显示正常信息
    if not tooltip_map:
        anomaly_explanations_html = """
        <div class="alert alert-success">
            <p>未检测到异常点。</p>
        </div>
        """
    
    # 构建方法摘要HTML
    methods_summary_html = ""
    for result in detection_results:
        methods_summary_html += f"""
        <div class="card mb-2">
            <div class="card-header">
                <strong>{result.method}</strong>
            </div>
            <div class="card-body">
                <p>{result.description}</p>
            </div>
        </div>
        """
    
    # 如果提供了分析文本，则转换成HTML格式
    formatted_text = ""
    if analysis_text:
        # 替换Markdown格式为HTML格式，避免使用复杂的f-string转义
        formatted_text = analysis_text.replace('\n', '<br>')
        formatted_text = re.sub(r'## (.*)', r'<h3>\1</h3>', formatted_text)
        formatted_text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', formatted_text)
        formatted_text = re.sub(r'- (.*)', r'• \1<br>', formatted_text)
    
    # 构建完整HTML报告
    html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>异常检测报告</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {{ padding: 20px; }}
        .iframe-container {{ width: 100%; height: 650px; border: none; }}
        .markdown-content {{ line-height: 1.6; }}
        .markdown-content h3 {{ margin-top: 20px; margin-bottom: 15px; color: #333; }}
        .markdown-content strong {{ font-weight: 600; color: #444; }}
    </style>
</head>
<body>
    <div class="container">
        <h1 class="mt-4 mb-4">时序数据异常检测报告</h1>
        
        <div class="card mb-4">
            <div class="card-header">
                <strong>用户问题</strong>
            </div>
            <div class="card-body">
                <p>{user_query}</p>
            </div>
        </div>
        
        <div class="card mb-4">
            <div class="card-header">
                <strong>分析结论</strong>
            </div>
            <div class="card-body">
                <div class="alert {alert_class}">
                    <h4>综合判定: {classification}</h4>
                    <p>综合得分: {composite_score:.2f}</p>
                </div>
                
                <!-- 将分析报告摘要内容放到分析结论中 -->
                <div class="markdown-content mt-3">
                    {formatted_text if analysis_text else ""}
                </div>
            </div>
        </div>
        
        <div class="card mb-4">
            <div class="card-header">
                <strong>异常检测图表</strong>
            </div>
            <div class="card-body p-0">
                <iframe class="iframe-container" src="{os.path.basename(chart_path)}"></iframe>
            </div>
        </div>
        
        <h2 class="mt-4 mb-3">异常点详细说明</h2>
        {anomaly_explanations_html}
        
        <h2 class="mt-4 mb-3">检测方法摘要</h2>
        {methods_summary_html}
        
        <footer class="mt-5 text-center text-muted">
            <p>生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </footer>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>"""

    # 写入文件
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html)

    return output_path

# === output/visualization.py ===
# output/visualization.py
import json
import uuid
import os
import time
from datetime import datetime
from typing import List, Dict, Tuple, Any, Optional

def generate_summary_echarts_html(series1, series2=None, detection_results=None, output_path=None, title="时序异常检测汇总图"):
    """
    生成融合所有方法的汇总异常检测可视化图表
    
    参数:
        series1: 第一个时序数据 [(timestamp, value), ...]
        series2: 可选，第二个时序数据，用于多序列对比
        detection_results: 多个检测器结果列表 [DetectionResult, ...]
        output_path: 输出HTML文件路径
        title: 图表标题
    
    返回:
        output_path: 输出的HTML文件路径
        tooltip_map: 异常点映射，用于报告中解释关联
    """
    if detection_results is None:
        detection_results = []
        
    chart_id = f"chart_summary_{uuid.uuid4().hex[:8]}"

    # 主序列数据 - 修改名称和数据格式
    series_list = [{
        "name": series2 is not None and "上周CPU利用率" or "原始序列",
        "type": "line",
        "data": [[t * 1000, v] for t, v in series1],
        "symbolSize": 0,  # 减小正常点的大小
        "lineStyle": {"width": 2},
        "itemStyle": {"color": "#5470C6"}
    }]
    
    # 添加第二个序列（如果存在）
    if series2 is not None:
        # 将两个序列的时间范围对齐
        min_time = min(series1[0][0], series2[0][0])
        offset1 = series1[0][0] - min_time  # 第一个序列相对于较早开始时间的偏移
        offset2 = series2[0][0] - min_time  # 第二个序列相对于较早开始时间的偏移
        
        # 将时间戳调整为相对时间，这样两个序列可以在图表上对齐
        adjusted_series2 = [(t - offset2 + offset1, v) for t, v in series2]
        
        series_list.append({
            "name": "这周CPU利用率",
            "type": "line",
            "data": [[t * 1000, v] for t, v in adjusted_series2],
            "symbolSize": 0,  # 减小正常点的大小
            "lineStyle": {"width": 2},
            "itemStyle": {"color": "#91CC75"}
        })
        
        # 添加差值曲线
        if len(series1) == len(adjusted_series2):
            diff_data = []
            for i in range(len(series1)):
                diff_data.append([series1[i][0] * 1000, series1[i][1] - adjusted_series2[i][1]])
                
            series_list.append({
                "name": "差值曲线",
                "type": "line",
                "data": diff_data,
                "symbolSize": 0,  # 减小正常点的大小
                "lineStyle": {"width": 1, "type": "dashed"},
                "itemStyle": {"color": "#EE6666"}
            })

    mark_points = []
    mark_areas = []
    extra_series = []
    tooltip_map = {}

    point_counter = 1  # 异常点序号
    explanation_counter = 1  # 解释编号

    for result in detection_results:
        # 处理点异常
        if result.visual_type == "point":
            for i, ts in enumerate(result.anomalies):
                value = next((v for t, v in series1 if t == ts), None)
                if value is None:
                    continue
                
                # 构建标签和提示信息
                label = f"异常点#{point_counter}\n方法:{result.method}"
                explanation = ""
                if i < len(result.explanation):
                    explanation = result.explanation[i]
                    label += f"\n解释#{explanation_counter}"
                    # 存储对应关系，用于报告生成
                    tooltip_map[point_counter] = {
                        "method": result.method,
                        "ts": ts,
                        "value": value,
                        "explanation": explanation
                    }
                    explanation_counter += 1
                
                # 添加标记点
                mark_points.append({
                    "coord": [ts * 1000, value],
                    "symbol": "circle",
                    "symbolSize": 7,
                    "itemStyle": {"color": "red"},
                    "label": {"formatter": f"#{point_counter}", "show": True, "position": "top"},
                    "tooltip": {"formatter": label}
                })
                point_counter += 1

        # 处理区间异常
        if result.visual_type in ("range", "curve"):
            for i, (start, end) in enumerate(result.intervals):
                # 获取区间解释
                area_explanation = ""
                if i < len(result.explanation):
                    area_explanation = result.explanation[i]
                
                # 添加标记区域
                mark_areas.append({
                    "itemStyle": {"color": "rgba(255, 100, 100, 0.2)"},
                    "label": {"show": True, "position": "top", "formatter": f"#{point_counter}"},
                    "tooltip": {"formatter": f"异常区间#{point_counter}\n方法:{result.method}\n{area_explanation}"},
                    "xAxis": start * 1000,
                    "xAxis2": end * 1000
                })
                
                # 存储对应关系
                tooltip_map[point_counter] = {
                    "method": result.method,
                    "ts_start": start,
                    "ts_end": end,
                    "explanation": area_explanation
                }
                
                point_counter += 1
                explanation_counter += 1

        # 处理辅助曲线 - 使用第二个y轴
        if result.visual_type == "curve" and result.auxiliary_curve:
            # 检查方法名来确定是否使用第二个Y轴
            use_second_yaxis = result.method in ["CUSUM", "TrendDriftCUSUM"]
            yAxisIndex = 1 if use_second_yaxis else 0
            
            curve_data = [[t * 1000, v] for t, v in result.auxiliary_curve]
            extra_series.append({
                "name": f"{result.method} 辅助曲线",
                "type": "line",
                "yAxisIndex": yAxisIndex,  # 使用右侧Y轴
                "data": curve_data,
                "lineStyle": {"type": "dashed", "width": 1.5},
                "itemStyle": {"color": "#EE6666"},
                "showSymbol": False
            })

    # 合并所有系列
    series_list.extend(extra_series)
    
    # 检查是否需要第二个Y轴
    need_second_yaxis = any(s.get("yAxisIndex", 0) == 1 for s in series_list)

    # 构建 ECharts 选项
    option = {
        "title": {"text": title, "left": "center"},
        "tooltip": {"trigger": "axis", "axisPointer": {"type": "cross"}},
        "legend": {"top": 30, "data": [s["name"] for s in series_list]},
        "grid": {"left": "3%", "right": need_second_yaxis and "8%" or "4%", "bottom": "3%", "containLabel": True},
        "toolbox": {
            "feature": {
                "saveAsImage": {},
                "dataZoom": {},
                "restore": {}
            }
        },
        "xAxis": {
            "type": "time",
            "name": "时间",
            "axisLabel": {"formatter": "{yyyy}-{MM}-{dd} {HH}:{mm}"}
        },
        "yAxis": [
            {"type": "value", "name": "数值", "position": "left"}
        ],
        "series": series_list,
        "dataZoom": [
            {"type": "slider", "show": True, "xAxisIndex": [0], "start": 0, "end": 100},
            {"type": "inside", "xAxisIndex": [0], "start": 0, "end": 100}
        ]
    }
    
    # 添加第二个Y轴（如果需要）
    if need_second_yaxis:
        option["yAxis"].append({
            "type": "value",
            "name": "辅助曲线值",
            "position": "right",
            "splitLine": {"show": False}
        })

    # 整合标记点和标记区域
    if mark_points:
        series_list[0]["markPoint"] = {"data": mark_points, "symbolSize": 8}
    
    if mark_areas:
        series_list[0]["markArea"] = {
            "data": [[{"xAxis": area["xAxis"], "itemStyle": area["itemStyle"]}, 
                     {"xAxis": area["xAxis2"]}] for area in mark_areas]
        }

    # 生成HTML
    html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>{title}</title>
    <script src="https://cdn.jsdelivr.net/npm/echarts@5/dist/echarts.min.js"></script>
</head>
<body>
    <div id="{chart_id}" style="width:100%; height:600px;"></div>
    <script>
        var chart = echarts.init(document.getElementById('{chart_id}'));
        var option = {json.dumps(option, ensure_ascii=False)};
        chart.setOption(option);
        window.addEventListener('resize', function() {{
            chart.resize();
        }});
    </script>
</body>
</html>"""

    # 确保输出目录存在
    if output_path:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # 写入文件
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html)
        return output_path, tooltip_map
    else:
        return html, tooltip_map


# 向后兼容函数
def generate_echarts_html_single(series, anomalies, title="单序列异常检测"):
    """
    向后兼容函数 - 生成单序列异常检测图表
    """
    from detectors.base import DetectionResult
    import time
    
    # 将旧格式转换为检测结果对象
    result = DetectionResult(
        method="Legacy",
        anomalies=anomalies,
        description="旧版接口生成的异常检测图表",
        visual_type="point"
    )
    
    path = f"output/plots/legacy_single_{int(time.time())}.html"
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    chart_path, _ = generate_summary_echarts_html(
        series, None, [result], path, title
    )
    
    return chart_path

def generate_echarts_html_multi(series1, series2, anomalies, title="多序列对比异常检测"):
    """
    向后兼容函数 - 生成多序列对比异常检测图表
    """
    from detectors.base import DetectionResult
    import time
    
    # 将旧格式转换为检测结果对象
    result = DetectionResult(
        method="Legacy",
        anomalies=anomalies,
        description="旧版接口生成的异常检测图表",
        visual_type="point"
    )
    
    path = f"output/plots/legacy_multi_{int(time.time())}.html"
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    chart_path, _ = generate_summary_echarts_html(
        series1, series2, [result], path, title
    )
    
    return chart_path

# === output/analysis_summary.py ===
# output/analysis_summary.py
import datetime
from typing import List, Dict, Any

def generate_text_analysis(detection_results: List, composite_score: float, classification: str, 
                          series_info: Dict, is_multi_series: bool = False) -> str:
    """
    生成易于理解的文字分析报告
    
    参数:
        detection_results: 检测结果列表
        composite_score: 综合得分
        classification: 分类结果 ('正常', '轻度异常', '高置信度异常')
        series_info: 关于数据的信息 {"ip": str, "field": str, "start": str, "end": str}
        is_multi_series: 是否是多序列分析
        
    返回:
        str: 结构化的文字分析报告
    """
    # 提取基本信息
    ip = series_info.get("ip", "未知主机")
    field = series_info.get("field", "未知指标")
    start_time = series_info.get("start", "未知开始时间")
    end_time = series_info.get("end", "未知结束时间")
    
    # 检测统计信息
    total_anomalies = sum(len(result.anomalies) for result in detection_results)
    total_intervals = sum(len(result.intervals) for result in detection_results)
    methods_with_anomalies = [r.method for r in detection_results if len(r.anomalies) > 0 or len(r.intervals) > 0]
    
    # 格式化时间
    try:
        start_dt = datetime.datetime.strptime(start_time, "%Y-%m-%d %H:%M:%S")
        end_dt = datetime.datetime.strptime(end_time, "%Y-%m-%d %H:%M:%S")
        period_desc = f"{start_dt.strftime('%Y年%m月%d日 %H:%M')} 至 {end_dt.strftime('%Y年%m月%d日 %H:%M')}"
    except:
        period_desc = f"{start_time} 至 {end_time}"
    
    # 根据评分生成总体分析
    if classification == "正常":
        summary_line = f"系统运行正常，未发现明显异常。"
        if total_anomalies > 0:
            summary_line += f" 虽然检测到 {total_anomalies} 个可能的异常点，但整体影响较小，综合评分为 {composite_score:.2f}，低于异常阈值。"
    elif classification == "轻度异常":
        summary_line = f"系统运行存在轻微异常，综合评分为 {composite_score:.2f}。"
        if total_anomalies > 0:
            summary_line += f" 共检测到 {total_anomalies} 个异常点，可能需要关注。"
    else:  # 高置信度异常
        summary_line = f"系统运行存在明显异常，综合评分为 {composite_score:.2f}，超过高置信阈值。"
        if total_anomalies > 0:
            summary_line += f" 共检测到 {total_anomalies} 个异常点，强烈建议进一步分析。"
    
    # 构建详细分析报告
    if is_multi_series:
        ip2 = series_info.get("ip2", "第二组数据")
        analysis_text = f"""## {ip} vs {ip2} {field} 对比分析报告

**分析时段**: {period_desc}

**总体分析**: {summary_line}

**检测方法详情**:
"""
    else:
        analysis_text = f"""## {ip} {field} 异常检测报告

**分析时段**: {period_desc}

**总体分析**: {summary_line}

**检测方法详情**:
"""

    # 添加各个检测方法的详细分析
    for result in detection_results:
        # 跳过无异常的方法
        if not result.anomalies and not result.intervals:
            continue
            
        method_desc = result.description
        
        # 处理不同类型的异常
        if result.visual_type == "point" and result.anomalies:
            analysis_text += f"\n- **{result.method}**: 检测到 {len(result.anomalies)} 个异常点。"
            
            # 添加代表性异常的解释
            if result.explanation and len(result.explanation) > 0:
                max_examples = min(3, len(result.explanation))
                analysis_text += " 典型异常表现为: "
                for i in range(max_examples):
                    # 尝试将时间戳转换为可读时间
                    try:
                        ts = result.anomalies[i]
                        ts_str = datetime.datetime.fromtimestamp(ts).strftime("%H:%M:%S")
                        analysis_text += f"{ts_str} - {result.explanation[i]}; "
                    except:
                        analysis_text += f"{result.explanation[i]}; "
        
        elif result.visual_type in ("range", "curve") and result.intervals:
            analysis_text += f"\n- **{result.method}**: 检测到 {len(result.intervals)} 个异常区间。"
            
            # 添加代表性区间的解释
            if result.explanation and len(result.explanation) > 0:
                max_examples = min(2, len(result.explanation))
                analysis_text += " 典型异常区间表现为: "
                for i in range(max_examples):
                    try:
                        start, end = result.intervals[i]
                        start_str = datetime.datetime.fromtimestamp(start).strftime("%H:%M:%S")
                        end_str = datetime.datetime.fromtimestamp(end).strftime("%H:%M:%S")
                        analysis_text += f"{start_str}至{end_str} - {result.explanation[i]}; "
                    except:
                        analysis_text += f"{result.explanation[i]}; "
        
        elif result.visual_type == "none" and result.explanation:
            analysis_text += f"\n- **{result.method}**: {' '.join(result.explanation[:2])}"
    
    # 添加建议和结论
    analysis_text += "\n\n**建议**:\n"
    
    if classification == "正常":
        analysis_text += "- 系统运行状态良好，可继续当前运维策略。"
    elif classification == "轻度异常":
        analysis_text += f"- 关注 {field} 指标的变化趋势，尤其是 {', '.join(methods_with_anomalies[:2])} 检测到的异常点。\n"
        analysis_text += "- 考虑设置该指标的监控告警，避免问题恶化。"
    else:  # 高置信度异常
        analysis_text += f"- 建议立即排查 {field} 指标异常的根本原因，特别是 {', '.join(methods_with_anomalies[:2])} 检测到的问题。\n"
        analysis_text += "- 检查系统配置和相关依赖组件状态。\n"
        analysis_text += "- 考虑与其他指标交叉分析，确定影响范围。"
    
    return analysis_text

# === output/__init__.py ===


# === test/testfile.py ===
import os

def dump_project_code(project_root, output_file="project_code_dump.txt"):
    with open(output_file, "w", encoding="utf-8") as out:
        for root, _, files in os.walk(project_root):
            if "__pycache__" in root:
                continue
            for file in files:
                if file.endswith(".py"):
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, project_root)
                    out.write(f"# === {rel_path} ===\n")
                    with open(file_path, "r", encoding="utf-8") as f:
                        out.write(f.read())
                    out.write("\n\n")

project_path = "/home/cnic/aiagent1"  
dump_project_code(project_path)


# === test/mail_test.py ===
#! /usr/bin/env python3
import re
import json
import datetime
import requests
from django.conf import settings

AIOPS_BACKEND_DOMAIN = 'https://aiopsbackend.cstcloud.cn'
LLM_URL = 'http://10.16.1.16:58000/v1/chat/completions'

AUTH = ('chelseyyycheng@outlook.com', 'UofV1uwHwhVp9tcTue')

tools = [
    {
        "type": "function",
        "function": {
            "name": "请求智能运管后端Api，获取指标项的时序数据",
            "description": "请求智能运管后端Api，获取指标项的时序数据",
            "parameters": {
                "type": "object",
                "properties": {
                    "ip": {
                        "type": "string",
                        "description": "要查询的ip"
                    },
                    "start": {
                        "type": "string",
                        "description": "日期，格式为 Y-%m-%d %H:%M:%S"
                    },
                    "end": {
                        "type": "string",
                        "description": "日期，格式为 Y-%m-%d %H:%M:%S"
                    },
                    "field": {
                        "type": "string",
                        "description": "监控项名称，只可在监控项列表中选择"
                    },
                },
                "required": ["ip", "start", "end", "field"],
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "请求智能运管后端Api，查询监控实例有哪些监控项",
            "description": "请求智能运管后端Api，查询监控实例有哪些监控项",
            "parameters": {
                "type": "object",
                "properties": {
                    "service": {
                        "type": "string",
                        "description": "要查询的系统名称"
                    },
                    "instance": {
                        "type": "string",
                        "description": "要查询的监控实例"
                    },
                },
                "required": ["service", "instance"],

            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "请求智能运管后端Api，查询监控实例之间的拓扑关联关系",
            "description": "监控实例上联了哪些监控实例列表，下联了哪些监控实例列表",
            "parameters": {
                "type": "object",
                "properties": {
                    "service": {
                        "type": "string",
                        "description": "要查询的系统名称"
                    },
                    "instance_ip": {
                        "type": "string",
                        "description": "要查询的监控实例的IP地址"
                    },
                },
                "required": ["service", 'instance_ip'],
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "请求智能运管后端Api，查询监控服务的资产情况和监控实例",
            "description": "请求智能运管后端Api，查询监控服务的资产情况和监控实例",
            "parameters": {
                "type": "object",
                "properties": {
                    "service": {
                        "type": "string",
                        "description": "要查询的系统名称"
                    },
                },
                "required": ["service"],

            }
        }
    },

]


def monitor_item_list(ip):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/machine/field/?instance={ip}'
    resp = requests.get(url=url, auth=AUTH)
    text = json.loads(resp.text)
    result = dict()
    if resp.status_code == 200:
        for item in text:
            result[item.get('field')] = item.get('purpose')
        return result
    else:
        return text


def get_service_asset(service):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/property/mail/?ordering=num_id&page=1&page_size=2000'
    resp = requests.get(url=url, auth=AUTH)
    text = json.loads(resp.text)
    results = text.get('results')
    item_list = []
    for _ in results:
        _['category'] = _.get('category').get('name')
        _["ip_set"] = [_.get("ip") for _ in _.get('ip_set')]
        _.pop('num_id')
        _.pop('creation')
        _.pop('modification')
        _.pop('remark')
        _.pop('sort_weight')
        _.pop('monitor_status')
        for k, v in _.copy().items():
            if not v or v == '无':
                _.pop(k)
        item_list.append(_)
    return item_list


def get_service_asset_edges(service, instance_ip):
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/property/mail/topology/search?instance={instance_ip}'
    resp = requests.get(url=url, auth=AUTH)
    text = json.loads(resp.text)
    # print(text)
    return text


def get_monitor_metric_value(ip, start, end, field):
    metric_field_list = monitor_item_list(ip)
    if field not in metric_field_list.keys():
        return f"未知的监控项：{field}"
    # 查询监控指标情况
    start_timestamp = datetime.datetime.strptime(start, "%Y-%m-%d %H:%M:%S").timestamp()
    end_timestamp = datetime.datetime.strptime(end, "%Y-%m-%d %H:%M:%S").timestamp()
    url = f'{AIOPS_BACKEND_DOMAIN}/api/v1/monitor/mail/metric/format-value/?start={start_timestamp}&end={end_timestamp}&instance={ip}&field={field}'
    resp = requests.get(url=url, auth=AUTH)
    text = resp.text
    text = json.loads(text)
    return text


# ------------------------------------------------------------------------


def llm_call(messages):
    # for _ in messages:
    #     print(_)
    #     print('\n')
    data = {
        "model": "Qwen2.5-14B-Instruct",
        "temperature": 0.1,
        "messages": messages,
    }
    response = requests.post(LLM_URL, json=data)
    if response.status_code == 200:
        response_data = response.json()
        if 'choices' in response_data and len(response_data['choices']) > 0:
            generated_content = response_data['choices'][0]['message']['content']
            # print('##Token使用情况##:\n\n', response_data['usage'])
            # print('------------------\n\n')
            return response_data['choices'][0]['message']
        else:
            print(response_data)
            raise Exception("模型没有返回信息")
    else:
        print(f'Error: {response.status_code}')
        print(response.text)


def init_message_by_role(role, content):
    message = {
        'role': role,
        "content": content
    }
    return message


def parse_llm_response(llm_resp_content):
    invalid_value = ["空", '无']
    thought_match = re.search(r'<思考过程>(.*)</思考过程>', llm_resp_content, re.S)
    if thought_match and thought_match[0] not in invalid_value:
        thought = thought_match.group(1)
    else:
        thought = ""
    action_match = re.search(r'<工具调用>(.*)</工具调用>', llm_resp_content)
    if action_match and action_match[0] not in invalid_value:
        action = action_match.group(1)
    else:
        action = ""
    action_input_match = re.search(r'<调用参数>(.*)</调用参数>', llm_resp_content)
    if action_input_match and action_input_match[0] not in invalid_value:
        action_input = action_input_match.group(1)
    else:
        action_input = ""
    if "<最终答案>" in llm_resp_content and "</最终答案>" not in llm_resp_content:
        llm_resp_content += "</最终答案>"
    final_answer_match = re.search(r'<最终答案>(.*)</最终答案>', llm_resp_content, re.S)
    if final_answer_match and final_answer_match[0] not in invalid_value:
        final_answer = final_answer_match.group(1)
    else:
        final_answer = ""
    result = {
        'thought': thought,
        'action': action,
        'action_input': action_input,
        'final_answer': final_answer,
    }
    return result


def react(llm_resp_content):
    is_final = False
    llm_parsed_dict = parse_llm_response(llm_resp_content)
    # print("##解析后的参数", llm_parsed_dict)
    action = llm_parsed_dict.get('action')
    action_input = llm_parsed_dict.get('action_input')
    final_answer = llm_parsed_dict.get('final_answer')
    # print("当前的大模型解析", llm_parsed_dict)
    if action and action_input:
        # print("##调用函数##", action, action_input)
        action_input = json.loads(action_input)
        if action == '请求智能运管后端Api，获取指标项的时序数据':
            return get_monitor_metric_value(**action_input), is_final
        if action == '请求智能运管后端Api，查询监控实例有哪些监控项':
            return monitor_item_list(action_input.get('instance')), is_final
        if action == '请求智能运管后端Api，查询监控服务的资产情况和监控实例':
            return get_service_asset(action_input.get('service')), is_final
        if action == "请求智能运管后端Api，查询监控实例之间的拓扑关联关系":
            return get_service_asset_edges(**action_input), is_final
    if final_answer:
        is_final = True
        return final_answer, is_final
    else:
        result = """
生成的文本格式有误，严格按照以下指定格式生成响应：
```
<思考过程>你的思考过程</思考过程>
<工具调用>工具名称（必须是{tool_names}之一），如果不调用工具，则为空</工具调用>
<调用参数>工具输入参数（严格符合工具描述格式）</调用参数>
<最终答案>用户问题的最终结果（知道问题的最终答案时返回）</最终答案>
```
"""
        return result, is_final


def stream_response_format(category, content):
    data = f'data: {json.dumps({category: content}, ensure_ascii=False)}\n\n'
    return data


def chat(user_content):
    system_template = '''你是一个严格遵守格式规范的用于运维功能，运维数据可视化，运行于生产环境的ReAct智能体，你叫小助手，必须按以下格式处理请求：

    可用工具：
    {tools}

    处理规则：
    1.根据用户的问题来自行判断是否要调用工具以及调用哪个工具
    2.每次只能调用一个工具
    3.不能伪造数据
    3.严格按照以下xml格式生成响应文本：
    ```
    <思考过程>你的思考过程</思考过程>
    <工具调用>工具名称（必须是{tool_names}之一），如果不调用工具，则为空</工具调用>
    <调用参数>工具输入参数（严格符合工具描述格式）</调用参数>
    <最终答案>用户问题的最终结果（知道问题的最终答案时返回）</最终答案>
    ```
    '''
    history = list()
    history.append(init_message_by_role(
        role='system',
        content=system_template.format(
            tools=json.dumps(tools, ensure_ascii=False),
            tool_names=json.dumps([tool["function"]["name"] for tool in tools], ensure_ascii=False), )
    ))
    current_datetime = str(datetime.datetime.now()).split('.')[0]
    history.append(init_message_by_role(role='user', content=f'当前时间是 {current_datetime}'))
    history.append(init_message_by_role(role='user', content=user_content))
    count = 1
    while True:
        print(f'第{count}次循环')
        llm_resp_message = llm_call(history)
        print('##大模型响应##', llm_resp_message)
        history.append(llm_resp_message)
        response, is_final_flag = react(llm_resp_message.get('content'))
        history.append(init_message_by_role(role='user', content=f"<工具调用结果>: {response}</工具调用结果>"))
        if is_final_flag:
            print(response)
            return
        count += 1
        if count >= 15:
            response = llm_resp_message.get('content')
            print(response)
            return


if __name__ == '__main__':
    # chat('你好')
    chat(
        '我想查询邮件系统 192.168.0.110 这台主机今天1点到1点30分的cpu利用率，并给出echarts折线图的完整html，并进行分析给出分析报告')


# === analysis/multi_series.py ===
# analysis/multi_series.py
import config
import math
import numpy as np
import logging
from utils.time_utils import group_anomaly_times
from detectors.base import DetectionResult

logger = logging.getLogger("anomaly_detection.multi_series")

def analyze_multi_series(series1, series2, align=True):
    """
    对两个时间序列进行对比分析
    
    参数:
        series1: 第一个时间序列
        series2: 第二个时间序列
        align: 是否对齐时间戳
        
    返回:
        dict: 包含分析结果的字典
    """
    # 输入参数验证
    if not series1 or not series2:
        logger.warning("输入时间序列为空")
        return {
            "method_results": [],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": [],
            "anomaly_intervals": []
        }
    
    # 导入放在函数内部避免循环引用
    from detectors.residual_comparison import ResidualComparisonDetector
    from detectors.trend_drift_cusum import TrendDriftCUSUMDetector
    from detectors.change_rate import ChangeRateDetector
    from detectors.trend_slope import TrendSlopeDetector
    from analysis.data_alignment import align_series
    
    # 检查两个序列是否有足够的差异
    values1 = [v for _, v in series1]
    values2 = [v for _, v in series2]
    
    if align:
        try:
            series1, series2 = align_series(series1, series2, method="linear", fill_value="extrapolate")
            logger.info("成功对齐两个时间序列")
        except Exception as e:
            logger.error(f"时间序列对齐失败: {e}")
            # 继续使用原始序列
    
    # 计算两个序列的相似度
    try:
        mean_abs_diff = np.mean(np.abs(np.array(values1) - np.array(values2)))
        relative_diff = mean_abs_diff / (np.mean(np.abs(values1)) + 1e-10)
        
        logger.info(f"两序列的相对差异: {relative_diff:.1%}")
        # 如果差异极小，可能不需要详细分析
        if relative_diff < 0.05:  # 小于5%的差异
            logger.info("序列几乎相同，无需详细分析")
            return {
                "method_results": [],
                "composite_score": 0,
                "classification": "正常",
                "anomaly_times": [],
                "anomaly_intervals": []
            }
    except Exception as e:
        logger.warning(f"计算序列差异失败: {e}")
        # 继续分析

    # 加载阈值配置
    thres = config.THRESHOLD_CONFIG
    
    # 执行各个检测方法
    detection_results = []
    
    # 1. 残差对比方法
    try:
        res_residual = ResidualComparisonDetector(
            threshold=thres.get("ResidualComparison", {}).get("threshold", 3.5)
        ).detect(series1, series2)
        detection_results.append(res_residual)
        logger.info(f"残差对比检测到 {len(res_residual.anomalies)} 个异常点")
    except Exception as e:
        logger.error(f"残差对比检测失败: {e}")
    
    # 2. 趋势漂移CUSUM方法
    try:
        res_drift = TrendDriftCUSUMDetector(
            threshold=thres.get("TrendDriftCUSUM", {}).get("drift_threshold", 8.0)
        ).detect(series1, series2)
        detection_results.append(res_drift)
        logger.info(f"趋势漂移检测到 {len(res_drift.intervals)} 个异常区间")
    except Exception as e:
        logger.error(f"趋势漂移检测失败: {e}")
    
    # 3. 变化率方法
    try:
        res_change = ChangeRateDetector(
            threshold=thres.get("ChangeRate", {}).get("threshold", 0.7)
        ).detect(series1, series2)
        detection_results.append(res_change)
        logger.info(f"变化率检测到 {len(res_change.explanation)} 个文本解释")
    except Exception as e:
        logger.error(f"变化率检测失败: {e}")
    
    # 4. 趋势斜率方法
    try:
        res_slope = TrendSlopeDetector(
            threshold=thres.get("TrendSlope", {}).get("slope_threshold", 0.4),
            window=thres.get("TrendSlope", {}).get("window", 5)
        ).detect(series1, series2)
        detection_results.append(res_slope)
        logger.info(f"趋势斜率检测到 {len(res_slope.anomalies)} 个异常点")
    except Exception as e:
        logger.error(f"趋势斜率检测失败: {e}")

    # 过滤无效结果
    method_results = [
        r for r in detection_results if r is not None
    ]
    
    if not method_results:
        logger.warning("所有检测方法都失败")
        return {
            "method_results": [],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": [],
            "anomaly_intervals": []
        }

    # 计算综合得分
    total_weight = 0.0
    composite_score = 0.0
    length = max(len(series1), len(series2)) or 1
    
    # 方法有意义的分数
    method_scores = {}

    for res in method_results:
        m_name = res.method
        weight = config.WEIGHTS_MULTI.get(m_name, 0.25)  # 默认权重0.25
        total_weight += weight
        
        # 计算方法得分
        if res.visual_type == "none" and res.explanation:
            # 纯文本解释型检测器
            # 根据解释的数量和内容评估得分
            has_significant_diff = any(("差异较大" in expl or "明显" in expl) 
                                      for expl in res.explanation)
            method_score = 0.4 if has_significant_diff else 0.2 if res.explanation else 0
        else:
            # 可视化型检测器
            anomaly_count = len(res.anomalies)
            interval_count = len(res.intervals) * 3  # 区间异常权重更高
            total_count = anomaly_count + interval_count
            
            if total_count > 0:
                # 计算异常比例
                if m_name == "TrendDriftCUSUM":
                    # TrendDriftCUSUM方法特殊处理
                    # 根据区间覆盖的时间比例来评估
                    total_duration = 0
                    for start, end in res.intervals:
                        total_duration += (end - start)
                    
                    coverage_ratio = total_duration / (series1[-1][0] - series1[0][0] + 1)
                    # 调整分数，减少过度敏感
                    if coverage_ratio > 0.5:  # 覆盖超过50%
                        method_score = min(0.7, 0.4 + 0.3 * coverage_ratio)
                    else:
                        method_score = 0.3 * coverage_ratio
                else:
                    # 其他方法使用对数缩放的点/区间数量
                    anomaly_ratio = total_count / length
                    if anomaly_ratio < 0.01:  # 低于1%的异常率
                        method_score = 0.2 + 0.3 * (anomaly_ratio * 100)  # 线性调整
                    else:
                        method_score = min(0.8, 0.2 + 0.3 * np.log10(1 + anomaly_ratio * 100))
            else:
                method_score = 0
        
        # 记录各方法得分
        method_scores[m_name] = method_score
        composite_score += weight * method_score
        logger.info(f"方法 {m_name} 得分: {method_score:.2f}, 权重: {weight}")

    if total_weight > 0:
        composite_score /= total_weight

    # 添加置信度调整
    methods_with_anomalies = sum(1 for res in method_results 
                              if (len(res.anomalies) > 0 or 
                                  len(res.intervals) > 0 or 
                                  (res.visual_type == "none" and len(res.explanation) > 0)))
    
    if methods_with_anomalies == 1 and len(method_results) > 1:
        logger.info("仅一个方法检测到异常，降低得分")
        composite_score *= 0.8  # 降低20%
    
    # 检查趋势漂移CUSUM的得分是否过高
    if "TrendDriftCUSUM" in method_scores and method_scores["TrendDriftCUSUM"] > 0.5:
        # 如果其他方法都没有明显异常，可能是误报
        other_methods_score = sum(score for name, score in method_scores.items() 
                                if name != "TrendDriftCUSUM") / max(1, len(method_scores) - 1)
        
        if other_methods_score < 0.2:  # 其他方法平均得分很低
            logger.warning("TrendDriftCUSUM可能误报，降低总得分")
            # 降低综合得分
            composite_score = (composite_score + other_methods_score) / 2

    # 确定分类
    classification = (
        "高置信度异常" if composite_score >= config.HIGH_ANOMALY_THRESHOLD
        else "轻度异常" if composite_score >= config.MILD_ANOMALY_THRESHOLD
        else "正常"
    )
    
    logger.info(f"综合得分: {composite_score:.2f}, 分类: {classification}")

    # 合并所有异常点
    all_anoms = set()
    for r in method_results:
        all_anoms.update(r.anomalies)
    anomaly_list = sorted(all_anoms)
    
    # 检查异常点是否太多（可能是误报）
    anomaly_ratio = len(anomaly_list) / length if length > 0 else 0
    if anomaly_ratio > 0.25:  # 超过25%是异常点
        logger.warning(f"异常点比例高达 {anomaly_ratio:.1%}，重新评估分类")
        # 对于大量异常点，需要多个方法一致确认才算高置信度异常
        if methods_with_anomalies < len(method_results) * 0.7:  # 少于70%的方法都检测到异常
            if classification == "高置信度异常":
                classification = "轻度异常"
                logger.info("降级为轻度异常")
            elif classification == "轻度异常" and anomaly_ratio > 0.4:
                classification = "正常"
                logger.info("降级为正常")
    
    # 分组时间区间
    intervals = group_anomaly_times(anomaly_list)

    return {
        "method_results": method_results,
        "composite_score": composite_score,
        "classification": classification,
        "anomaly_times": anomaly_list,
        "anomaly_intervals": intervals
    }

# === analysis/data_alignment.py ===
# 文件: analysis/data_alignment.py
import numpy as np
from scipy.interpolate import interp1d

def align_series(series1, series2, method="linear", fill_value="extrapolate"):
    
    #使用SciPy的interp1d进行插值，将series1和series2在相同的时间戳上对齐。
    if not series1 or not series2:
        return series1, series2

    s1_sorted = sorted(series1, key=lambda x: x[0])
    s2_sorted = sorted(series2, key=lambda x: x[0])

    t1 = np.array([row[0] for row in s1_sorted], dtype=np.float64)
    v1 = np.array([row[1] for row in s1_sorted], dtype=np.float64)
    t2 = np.array([row[0] for row in s2_sorted], dtype=np.float64)
    v2 = np.array([row[1] for row in s2_sorted], dtype=np.float64)

    all_ts = np.union1d(t1, t2)

    f1 = interp1d(t1, v1, kind=method, fill_value=fill_value, bounds_error=False)
    f2 = interp1d(t2, v2, kind=method, fill_value=fill_value, bounds_error=False)

    new_v1 = f1(all_ts)
    new_v2 = f2(all_ts)

    s1_aligned = [[int(ts), float(val)] for ts, val in zip(all_ts, new_v1)]
    s2_aligned = [[int(ts), float(val)] for ts, val in zip(all_ts, new_v2)]

    return s1_aligned, s2_aligned


# === analysis/__init__.py ===
# analysis/__init__.py
"""
分析模块，包含单序列和多序列时序数据分析功能
"""
from analysis.single_series import analyze_single_series
from analysis.multi_series import analyze_multi_series
from analysis.data_alignment import align_series

# === analysis/single_series.py ===
# analysis/single_series.py
import config
import numpy as np
import logging
from detectors.base import DetectionResult

logger = logging.getLogger("anomaly_detection.single_series")

def analyze_single_series(series):
    """
    对单个时间序列进行异常检测分析
    
    参数:
        series: 时间序列数据 [(timestamp, value), ...]
        
    返回:
        dict: 包含分析结果的字典
    """
    if not series:
        logger.warning("输入时间序列为空")
        return {
            "method_results": [],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": []
        }
        
    # 检查数据有效性
    values = [v for _, v in series]
    if len(set(values)) <= 1:
        logger.info("输入时间序列几乎不变，无需检测")
        return {
            "method_results": [],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": []
        }
        
    # 导入检测器
    from detectors.zscore import ZScoreDetector
    from detectors.cusum import CUSUMDetector
    
    # 加载阈值配置
    thres = config.THRESHOLD_CONFIG
    
    # 执行异常检测 - Z-Score
    try:
        res_z = ZScoreDetector(
            threshold=thres.get("Z-Score", {}).get("threshold", 3.5)
        ).detect(series)
        logger.info(f"Z-Score 检测到 {len(res_z.anomalies)} 个异常点")
    except Exception as e:
        logger.error(f"Z-Score 检测失败: {e}")
        res_z = DetectionResult(method="Z-Score", description=f"检测失败: {e}")
    
    # 执行异常检测 - CUSUM
    try:
        res_cusum = CUSUMDetector(
            drift_threshold=thres.get("CUSUM", {}).get("drift_threshold", 6.0),
            k=thres.get("CUSUM", {}).get("k", 0.7)
        ).detect(series)
        logger.info(f"CUSUM 检测到 {len(res_cusum.anomalies)} 个异常点, {len(res_cusum.intervals)} 个异常区间")
    except Exception as e:
        logger.error(f"CUSUM 检测失败: {e}")
        res_cusum = DetectionResult(method="CUSUM", description=f"检测失败: {e}")
    
    # 检查是否所有方法都失败了
    valid_results = []
    for result in [res_z, res_cusum]:
        if len(result.anomalies) > 0 or len(result.intervals) > 0 or result.visual_type != "none":
            valid_results.append(result)
    
    if not valid_results:
        logger.warning("所有检测方法都未找到异常或失败")
        return {
            "method_results": [res_z, res_cusum],
            "composite_score": 0,
            "classification": "正常",
            "anomaly_times": []
        }
    
    # 避免重复计算
    method_results = []
    method_names = set()
    
    for result in [res_z, res_cusum]:
        # 跳过重复方法
        if result.method in method_names:
            logger.info(f"跳过重复的 {result.method} 结果")
            continue
        method_results.append(result)
        method_names.add(result.method)
    
    # 计算综合得分
    total_weight = 0.0
    composite_score = 0.0
    length = len(series)
    
    for res in method_results:
        # 获取方法权重，确保每个方法都有权重值
        m_name = res.method
        weight = config.WEIGHTS_SINGLE.get(m_name, 0.3)  # 默认0.3
        total_weight += weight
        
        # 计算异常严重程度
        anomalies_count = len(res.anomalies)
        intervals_count = len(res.intervals) * 3  # 区间异常权重更高
        total_count = anomalies_count + intervals_count
        
        if res.visual_type == "none":
            # 纯文本解释类检测器
            method_score = 0
        elif total_count > 0:
            # 使用对数缩放，避免大数据集中的稀释效应
            # 调整公式，使得即使异常比例很小，也能得到一定的分数
            if total_count / length < 0.01:  # 低于1%的异常点
                method_score = 0.2 + 0.3 * (total_count / length) * 100  # 线性调整
            else:
                ratio = total_count / length
                method_score = min(0.9, 0.2 + 0.3 * np.log10(1 + ratio * 100))
        else:
            method_score = 0
        
        logger.info(f"方法 {m_name} 得分: {method_score:.2f}, 权重: {weight}")
        composite_score += weight * method_score
    
    if total_weight > 0:
        composite_score /= total_weight
    
    # 添加得分的置信度调整 - 减少误报
    # 如果只有一个方法检测到异常，降低得分
    methods_with_anomalies = sum(1 for res in method_results 
                              if len(res.anomalies) > 0 or len(res.intervals) > 0)
    if methods_with_anomalies == 1 and len(method_results) > 1:
        logger.info("仅一个方法检测到异常，降低得分")
        composite_score *= 0.8  # 降低20%
    
    # 确定分类
    classification = (
        "高置信度异常" if composite_score >= config.HIGH_ANOMALY_THRESHOLD
        else "轻度异常" if composite_score >= config.MILD_ANOMALY_THRESHOLD
        else "正常"
    )
    
    logger.info(f"综合得分: {composite_score:.2f}, 分类: {classification}")
    
    # 合并所有异常点
    all_anomalies = set()
    for r in method_results:
        all_anomalies.update(r.anomalies)
    
    # 计算异常点占总数据的比例
    anomaly_ratio = len(all_anomalies) / length if length > 0 else 0
    # 如果异常点超过25%，可能是误报，重新评估
    if anomaly_ratio > 0.25:
        logger.warning(f"异常点比例高达 {anomaly_ratio:.1%}，重新评估分类")
        # 对于大量异常点，需要多个方法一致确认才算高置信度异常
        if methods_with_anomalies < len(method_results) * 0.7:  # 少于70%的方法检测到异常
            if classification == "高置信度异常":
                classification = "轻度异常"
                logger.info("降级为轻度异常")
            elif classification == "轻度异常" and anomaly_ratio > 0.4:
                classification = "正常"
                logger.info("降级为正常")
    
    return {
        "method_results": method_results,
        "composite_score": composite_score,
        "classification": classification,
        "anomaly_times": sorted(all_anomalies)
    }

# === detectors/trend_drift_cusum.py ===
# detectors/trend_drift_cusum.py
import numpy as np
from detectors.base import DetectionResult
from utils.time_utils import group_anomaly_times

class TrendDriftCUSUMDetector:
    def __init__(self, threshold: float = 5.0):
        """
        初始化趋势漂移检测器
        
        参数:
            threshold: CUSUM阈值，超过此值视为异常
        """
        self.threshold = threshold
    
    def detect(self, series1: list, series2: list) -> DetectionResult:
        """
        检测两个时间序列之间的趋势漂移
        """
        if not series1 or not series2 or len(series1) < 10 or len(series2) < 10:
            return DetectionResult(
                method="TrendDriftCUSUM",
                description="数据点不足进行趋势漂移分析(至少需要10个点)",
                visual_type="none"
            )
        
        # 计算两个序列之间的差异
        residuals = []
        timestamps = [t for t, _ in series1]
        values1 = [v for _, v in series1]
        values2 = [v for _, v in series2]
        
        for (_, v1), (_, v2) in zip(series1, series2):
            residuals.append(v1 - v2)
        
        # 计算基本统计量，用于初步评估差异
        mean_abs_residual = np.mean(np.abs(residuals))
        max_abs_residual = np.max(np.abs(residuals))
        relative_diff = mean_abs_residual / (np.mean(np.abs(values1)) + 1e-10)
        
        # 检查整体差异是否足够大
        if max_abs_residual < 0.05 and mean_abs_residual < 0.01:
            return DetectionResult(
                method="TrendDriftCUSUM",
                description=f"两序列几乎相同，最大差异仅{max_abs_residual:.3f}，平均差异{mean_abs_residual:.3f}",
                visual_type="none"
            )
        
        # 相对差异小于10%
        if relative_diff < 0.1:
            return DetectionResult(
                method="TrendDriftCUSUM",
                description=f"两序列差异不显著(相对差异{relative_diff:.1%})，无需进行漂移分析",
                visual_type="none"
            )
        
        # 计算CUSUM前先对数据进行平滑处理
        def smooth_data(data, window=3):
            """简单移动平均平滑"""
            if len(data) < window:
                return data
            smoothed = np.convolve(data, np.ones(window)/window, mode='same')
            # 边缘处理
            smoothed[:window//2] = data[:window//2]
            smoothed[-window//2:] = data[-window//2:]
            return smoothed
        
        # 平滑残差
        smoothed_residuals = smooth_data(residuals, window=5)
        
        # 计算残差的标准差和均值
        mean = np.mean(smoothed_residuals)
        std = np.std(smoothed_residuals) 
        
        # 防止除零
        if std < 1e-10:
            std = 1.0
            
        # 标准化残差
        norm_residuals = [(r - mean) / std for r in smoothed_residuals]
        
        # 计算CUSUM，增大控制因子
        control_factor = 1.0  # 增大控制因子，减少灵敏度
        cum_sum_pos = [0]
        cum_sum_neg = [0]
        
        # 分别跟踪上升和下降趋势
        for r in norm_residuals:
            cum_sum_pos.append(max(0, cum_sum_pos[-1] + r - control_factor))
            cum_sum_neg.append(max(0, cum_sum_neg[-1] - r - control_factor))
        
        cum_sum_pos = cum_sum_pos[1:]  # 移除初始0
        cum_sum_neg = cum_sum_neg[1:]  # 移除初始0
        
        # 取两个方向CUSUM的最大值
        cum_sum = [max(p, n) for p, n in zip(cum_sum_pos, cum_sum_neg)]
        
        # 找出超过阈值的点，但增加持续性要求
        anomalies = []
        scores = []
        consecutive_count = 0
        required_consecutive = 3  # 至少需要连续3个点超过阈值
        
        for i, c in enumerate(cum_sum):
            if c > self.threshold:
                consecutive_count += 1
                if consecutive_count >= required_consecutive:
                    # 只将第一个连续异常点加入列表
                    if consecutive_count == required_consecutive:
                        anomalies.append(timestamps[i - required_consecutive + 1])
                        scores.append(float(c))
                    # 将当前点也加入
                    anomalies.append(timestamps[i])
                    scores.append(float(c))
            else:
                consecutive_count = 0
        
        # 分组为区间，要求至少5分钟
        intervals = group_anomaly_times(anomalies, max_gap=300)  # 5分钟间隔
        
        # 过滤短区间和弱区间
        filtered_intervals = []
        explanations = []
        
        for interval in intervals:
            start, end = interval
            duration = end - start
            
            # 获取区间内的CUSUM值
            interval_indices = [i for i, ts in enumerate(timestamps) if start <= ts <= end]
            if not interval_indices:
                continue
                
            interval_scores = [cum_sum[i] for i in interval_indices]
            avg_score = np.mean(interval_scores) if interval_scores else 0
            max_score = np.max(interval_scores) if interval_scores else 0
                
            # 过滤条件：区间至少5分钟且平均CUSUM值显著高于阈值
            # 同时最大值也要显著高于阈值
            if duration >= 300 and avg_score > self.threshold * 1.3 and max_score > self.threshold * 1.5:
                filtered_intervals.append(interval)
                explanations.append(
                    f"区间{formatted_timestamp(start)}至{formatted_timestamp(end)}的CUSUM值平均为{avg_score:.1f}，最大值{max_score:.1f}，超过阈值{self.threshold}，表明两序列存在持续趋势差异"
                )
        
        # 如果没有异常区间，返回无异常结果
        if not filtered_intervals:
            return DetectionResult(
                method="TrendDriftCUSUM",
                description=f"趋势漂移检测未发现明显的持续性异常区段",
                visual_type="none"
            )
        
        # 构建辅助曲线数据
        aux_curve = [(timestamps[i], float(cum_sum[i])) for i in range(len(timestamps))]
        
        return DetectionResult(
            method="TrendDriftCUSUM",
            anomalies=[],  # 不使用点异常，只用区间
            anomaly_scores=[],
            intervals=filtered_intervals,
            auxiliary_curve=aux_curve,
            description=f"趋势漂移检测发现 {len(filtered_intervals)} 个明显异常区段，相对差异{relative_diff:.1%}",
            visual_type="range",
            explanation=explanations
        )

def formatted_timestamp(ts):
    """将时间戳格式化为可读时间"""
    from datetime import datetime
    try:
        return datetime.fromtimestamp(ts).strftime("%H:%M:%S")
    except:
        return str(ts)

# === detectors/base.py ===
# detectors/base.py
from typing import List, Tuple, Optional, Dict, Any, Union

class DetectionResult:
    """
    统一的异常检测结果类，支持多种可视化展示方式
    
    属性:
        method: 检测方法名称
        anomalies: 异常时间点列表
        anomaly_scores: 对应每个异常点的分数
        intervals: 异常区间列表 [(start, end), ...]
        auxiliary_curve: 辅助曲线数据 [(ts, value), ...]
        description: 检测方法的描述
        visual_type: 可视化类型 "point"|"range"|"curve"|"none"
        explanation: 对应每个异常点/区间的解释文本
    """
    def __init__(
        self,
        method: str,
        anomalies: Optional[List[int]] = None,
        anomaly_scores: Optional[List[float]] = None,
        intervals: Optional[List[Tuple[int, int]]] = None,
        auxiliary_curve: Optional[List[Tuple[int, float]]] = None,
        description: str = "",
        visual_type: str = "point",  # point | range | curve | none
        explanation: Optional[List[str]] = None,
    ):
        self.method = method
        self.anomalies = anomalies or []
        self.anomaly_scores = anomaly_scores or []
        self.intervals = intervals or []
        self.auxiliary_curve = auxiliary_curve or []
        self.description = description
        self.visual_type = visual_type
        self.explanation = explanation or []
    
    def to_dict(self) -> Dict[str, Any]:
        """将结果转换为字典格式，便于序列化和传输"""
        return {
            "method": self.method,
            "anomalies": self.anomalies,
            "anomaly_scores": self.anomaly_scores,
            "intervals": self.intervals,
            "auxiliary_curve": self.auxiliary_curve,
            "description": self.description,
            "visual_type": self.visual_type,
            "explanation": self.explanation
        }

# === detectors/residual_comparison.py ===
import numpy as np
from detectors.base import DetectionResult

class ResidualComparisonDetector:
    def __init__(self, threshold: float = 3.0):
        self.threshold = threshold

    def detect(self, series1: list[tuple[int, float]], series2: list[tuple[int, float]]) -> DetectionResult:
        residuals = []
        timestamps = [t for t, _ in series1]

        for (_, v1), (_, v2) in zip(series1, series2):
            residuals.append(v1 - v2)

        residuals = np.array(residuals)
        mean = np.mean(residuals)
        std = np.std(residuals)
        z_scores = (residuals - mean) / std

        anomalies = []
        scores = []
        explanations = []

        for i, z in enumerate(z_scores):
            if abs(z) > self.threshold:
                anomalies.append(timestamps[i])
                scores.append(round(abs(z), 3))
                explanations.append(f"残差Z值={z:.2f}，两序列差异大")

        return DetectionResult(
            method="ResidualComparison",
            anomalies=anomalies,
            anomaly_scores=scores,
            description=f"基于残差Z分数检测出 {len(anomalies)} 个异常点（阈值={self.threshold}）",
            visual_type="point",
            explanation=explanations
        )


# === detectors/zscore.py ===
# detectors/zscore.py
import numpy as np
from typing import List, Tuple, Optional
from detectors.base import DetectionResult

class ZScoreDetector:
    """
    Z-Score 异常检测器，基于标准差倍数检测异常点
    """
    def __init__(self, threshold: float = 3.0):
        """
        初始化Z-Score检测器
        
        参数:
            threshold: Z-Score阈值，超过此值视为异常
        """
        self.threshold = threshold
    
    def detect(self, series: List[Tuple[int, float]]) -> DetectionResult:
        """
        对时间序列执行Z-Score检测
        
        参数:
            series: 时间序列数据 [(timestamp, value), ...]
            
        返回:
            DetectionResult: 检测结果对象
        """
        if not series:
            return DetectionResult(
                method="Z-Score",
                description="无数据进行Z-Score分析",
                visual_type="none"
            )
        
        # 提取时间戳和值
        timestamps = [t for t, _ in series]
        values = np.array([v for _, v in series])
        
        # 计算均值和标准差
        mean = np.mean(values)
        std = np.std(values) if len(values) > 1 else 1.0
        
        # 计算Z-Score
        z_scores = (values - mean) / std if std > 0 else np.zeros_like(values)
        
        # 找出超过阈值的异常点
        anomalies = []
        scores = []
        explanations = []
        
        for i, z in enumerate(z_scores):
            if abs(z) > self.threshold:
                anomalies.append(timestamps[i])
                scores.append(float(abs(z)))
                # 解释是高于均值还是低于均值
                direction = "高于" if z > 0 else "低于"
                explanations.append(f"Z-Score={z:.2f}，{direction}均值{abs(z):.2f}个标准差")
        
        return DetectionResult(
            method="Z-Score",
            anomalies=anomalies,
            anomaly_scores=scores,
            description=f"使用Z-Score方法(阈值={self.threshold})检测到{len(anomalies)}个异常点",
            visual_type="point",
            explanation=explanations
        )

# === detectors/__init__.py ===
# detectors/__init__.py
"""
异常检测器模块，包含各种用于时序数据异常检测的算法
"""

# 明确导出检测器类以简化导入
from detectors.zscore import ZScoreDetector
from detectors.cusum import CUSUMDetector
from detectors.residual_comparison import ResidualComparisonDetector
from detectors.trend_drift_cusum import TrendDriftCUSUMDetector
from detectors.change_rate import ChangeRateDetector
from detectors.trend_slope import TrendSlopeDetector
from detectors.base import DetectionResult

# 为向后兼容性导出旧的函数名
from detectors.zscore import ZScoreDetector as detect_zscore
from detectors.cusum import CUSUMDetector as detect_cusum
from detectors.residual_comparison import ResidualComparisonDetector as detect_residual_compare
from detectors.trend_drift_cusum import TrendDriftCUSUMDetector as detect_trend_drift
from detectors.change_rate import ChangeRateDetector as detect_change_rate
from detectors.trend_slope import TrendSlopeDetector as detect_trend_slope

# === detectors/trend_slope.py ===
# detectors/trend_slope.py
import numpy as np
from detectors.base import DetectionResult

class TrendSlopeDetector:
    def __init__(self, window: int = 5, threshold: float = 0.2, slope_threshold: float = None):
        """
        初始化趋势斜率检测器
        
        参数:
            window: 滑动窗口大小
            threshold: 斜率差异阈值 (与slope_threshold相同，为向后兼容)
            slope_threshold: 斜率差异阈值（如果提供，优先使用）
        """
        self.window = window
        # 支持两种参数名
        self.threshold = slope_threshold if slope_threshold is not None else threshold
    
    def detect(self, series1: list, series2: list) -> DetectionResult:
        """
        检测两个时间序列之间的趋势斜率差异
        
        参数:
            series1: 第一个时间序列 [(timestamp, value), ...]
            series2: 第二个时间序列 [(timestamp, value), ...]
            
        返回:
            DetectionResult: 检测结果对象
        """
        if not series1 or not series2 or len(series1) < self.window or len(series2) < self.window:
            return DetectionResult(
                method="TrendSlope",
                description="数据点不足进行趋势斜率分析",
                visual_type="none"
            )
            
        def calc_slope(values):
            x = np.arange(len(values))
            A = np.vstack([x, np.ones(len(values))]).T
            m, _ = np.linalg.lstsq(A, values, rcond=None)[0]
            return m

        slopes1, slopes2, timestamps = [], [], []

        for i in range(len(series1) - self.window + 1):
            window1 = [v for _, v in series1[i:i + self.window]]
            window2 = [v for _, v in series2[i:i + self.window]]
            
            try:
                slope1 = calc_slope(window1)
                slope2 = calc_slope(window2)
                slopes1.append(slope1)
                slopes2.append(slope2)
                timestamps.append(series1[i + self.window // 2][0])
            except Exception as e:
                print(f"计算斜率时出错: {e}")
                continue

        if not timestamps:
            return DetectionResult(
                method="TrendSlope",
                description="无法计算有效的趋势斜率",
                visual_type="none"
            )

        slope_diff = np.abs(np.array(slopes1) - np.array(slopes2))
        sorted_indices = np.argsort(-slope_diff)
        
        # 找出差异最大的几个点
        anomalies = []
        scores = []
        explanations = []
        
        for i in sorted_indices[:min(3, len(sorted_indices))]:
            ts = timestamps[i]
            diff = slope_diff[i]
            if diff > self.threshold:
                anomalies.append(ts)
                scores.append(float(diff))
                explanations.append(f"趋势斜率差值为 {diff:.3f}，高于阈值 {self.threshold}")
        
        return DetectionResult(
            method="TrendSlope",
            anomalies=anomalies,
            anomaly_scores=scores,
            description=f"TrendSlope 检测两个序列在滑动窗口下的局部趋势方向差异，发现 {len(anomalies)} 个异常点",
            visual_type="point",
            explanation=explanations
        )

# === detectors/change_rate.py ===
import numpy as np
from detectors.base import DetectionResult

class ChangeRateDetector:
    def __init__(self, threshold: float = 0.1):
        self.threshold = threshold

    def detect(self, series1: list[tuple[int, float]], series2: list[tuple[int, float]]) -> DetectionResult:
        timestamps = [t for t, _ in series1]
        rate_diffs = []

        for i in range(1, len(series1)):
            delta1 = series1[i][1] - series1[i - 1][1]
            delta2 = series2[i][1] - series2[i - 1][1]
            rate_diff = abs(delta1 - delta2)
            rate_diffs.append((timestamps[i], rate_diff))

        # 仅文字解释，找出偏差较大的前几段
        sorted_diff = sorted(rate_diffs, key=lambda x: -x[1])
        top = sorted_diff[:3]
        explanations = [
            f"{ts}: 变化速率差值为 {round(diff, 3)}，差异较大"
            for ts, diff in top
        ]

        return DetectionResult(
            method="ChangeRate",
            description="ChangeRate 用于比较两个序列的局部变化速度，检测出速率差异较大的时间点",
            visual_type="none",
            explanation=explanations
        )


# === detectors/cusum.py ===
# detectors/cusum.py
import numpy as np
from typing import List, Tuple, Optional
from detectors.base import DetectionResult

class CUSUMDetector:
    """
    CUSUM (累积和) 检测器，用于检测时间序列的累积偏移
    """
    def __init__(self, drift_threshold: float = 5.0, k: float = 0.5):
        """
        初始化CUSUM检测器
        
        参数:
            drift_threshold: CUSUM阈值，超过此值视为异常
            k: 灵敏度参数，较小的值对小偏移更敏感
        """
        self.drift_threshold = drift_threshold
        self.k = k
    
    def detect(self, series: List[Tuple[int, float]]) -> DetectionResult:
        """
        对时间序列执行CUSUM检测
        
        参数:
            series: 时间序列数据 [(timestamp, value), ...]
            
        返回:
            DetectionResult: 检测结果对象
        """
        if not series:
            return DetectionResult(
                method="CUSUM",
                description="无数据进行CUSUM分析",
                visual_type="none"
            )
        
        # 提取时间戳和值
        timestamps = [t for t, _ in series]
        values = np.array([v for _, v in series])
        
        # 计算数据平均值
        mean = np.mean(values)
        std = np.std(values) if len(values) > 1 else 1.0
        
        # 初始化CUSUM值
        cusum_pos = np.zeros(len(values))
        cusum_neg = np.zeros(len(values))
        
        # 计算正负CUSUM
        for i in range(1, len(values)):
            # 正向累积
            cusum_pos[i] = max(0, cusum_pos[i-1] + (values[i] - mean)/std - self.k)
            # 负向累积
            cusum_neg[i] = max(0, cusum_neg[i-1] - (values[i] - mean)/std - self.k)
        
        # 合并正负CUSUM
        cusum_combined = np.maximum(cusum_pos, cusum_neg)
        
        # 找出超过阈值的异常点
        anomalies = []
        scores = []
        for i, c in enumerate(cusum_combined):
            if c > self.drift_threshold:
                anomalies.append(timestamps[i])
                scores.append(float(c))
        
        # 生成解释文本
        explanations = [
            f"CUSUM值={scores[i]:.2f}，累计偏移超过阈值({self.drift_threshold})"
            for i in range(len(anomalies))
        ]
        
        # 构建区间
        from analysis.multi_series import group_anomaly_times
        intervals = group_anomaly_times(anomalies)
        
        # 构建CUSUM曲线数据
        cum_curve = [(timestamps[i], float(cusum_combined[i])) for i in range(len(timestamps))]
        
        return DetectionResult(
            method="CUSUM",
            anomalies=anomalies,
            anomaly_scores=scores,
            intervals=intervals,
            auxiliary_curve=cum_curve,
            description=f"CUSUM累积偏移检测到 {len(intervals)} 个异常区段，共 {len(anomalies)} 个高偏移点",
            visual_type="curve",
            explanation=explanations
        )

# === storage/database.py ===
import json
import config
try:
    import pymysql
except ImportError:
    pymysql = None

def save_analysis_record(record):
    """
    保存分析记录到 MySQL 数据库。
    record: dict 包含 ip, field, start_time, end_time, methods, anomalies, composite_score, classification, report 等
    """
    ip = record.get("ip")
    field = record.get("field")
    start = record.get("start_time")
    end = record.get("end_time")
    methods = record.get("methods")
    anomalies = record.get("anomaly_times")
    composite_score = record.get("composite_score")
    classification = record.get("classification")
    report = record.get("report")
    methods_json = json.dumps(methods, ensure_ascii=False)
    anomalies_json = json.dumps(anomalies, ensure_ascii=False)
    insert_sql = (
        "INSERT INTO anomaly_analysis_records "
        "(ip, field, start_time, end_time, methods, anomalies, composite_score, classification, report) "
        "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)"
    )
    values = (ip, field, start, end, methods_json, anomalies_json, composite_score, classification, report)
    try:
        if pymysql is None:
            from django.db import connection
            with connection.cursor() as cursor:
                cursor.execute(insert_sql, values)
                connection.commit()
        else:
            conn = pymysql.connect(host=config.DB_CONFIG['HOST'],
                                   port=config.DB_CONFIG['PORT'],
                                   user=config.DB_CONFIG['USER'],
                                   password=config.DB_CONFIG['PASSWORD'],
                                   database=config.DB_CONFIG['NAME'],
                                   charset='utf8mb4')
            cursor = conn.cursor()
            cursor.execute(insert_sql, values)
            conn.commit()
            cursor.close()
            conn.close()
    except Exception as e:
        print(f"保存记录失败: {e}")


